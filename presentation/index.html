<!doctype html>
<html lang="en">
<head>

    <meta charset="utf-8">
    <title>ExAI Projektpräsentation</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/reveal.min.css">
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/theme/black.min.css">
    <style>
        .reveal h1, .reveal h2, .reveal h3 {
            text-transform: none;
            color: #f0f0f0;
        }
        .reveal .slides {
            text-align: left;
        }
        .reveal img {
            margin: 15px 0;
            border-radius: 5px;
        }
        .reveal table {
            margin: 0 auto;
            border-collapse: collapse;
        }
        .reveal th, .reveal td {
            padding: 8px 15px;
            border: 1px solid #555;
        }
        .reveal pre code {
            padding: 15px;
            font-size: 0.8em;
        }
        .title-slide h1 {
            margin-top: 50px;
            font-size: 2.5em;
        }
        .title-slide h3 {
            margin-bottom: 50px;
            font-size: 1.3em;
            color: #ccc;
        }
    </style>
</head>
<body>
<div class="reveal">
    <div class="slides">

        <section class="title-slide">
            <h1>Explainable AI mit Bilddaten</h1>
            <h3>Unterscheidung von Pembroke und Cardigan Welsh Corgis</h3>
            <p>Lukas, Janik, Robin, Felix</p>
            <p><small>DHBW - ExAI Projekt</small></p>
        </section>

        <section class="title-slide">
            <h1>Agenda</h1>
            <ul>
                <li> Zielsetzung </li>
                <li> Datensatz-Auswahl & Preprocessing </li>
                <li> Modelltraining </li>
                <li> XAI Verfahren </li>
                <li> Notebook </li>
                <li> Analyse & Kritische Diskussion </li>
            </ul>
        </section>

        <section>
            <h1>Zielsetzung</h1>
        </section>

        <section>
            <h2>Problemstellung</h2>
            <p>Unterscheidung der Corgi-Rassen <strong>Pembroke</strong> und <strong>Cardigan</strong> mit Hilfe eines CNN.</p>
        </section>

        <section>
            <h2>Untersuchung</h2>
            <p>Beobachtung des Verhaltens bei Input von Mischlingen</p>
            <p>Ziel: Erklärbare Entscheidungen durch XAI-Methoden</p>
        </section>

        <section>
            <h1>Datensatz-Auswahl & Preprocessing</h1>
        </section>

        <section>
            <h2>Datensatz</h2>
            <ul>
                <li><strong>Stanford Dogs Dataset</strong></li>
                <li>120 Hunderassen, über 20.000 Bilder</li>
                <li>Verwendung der Klassen: Pembroke & Cardigan Welsh Corgi</li>
                <li><a href="http://vision.stanford.edu/aditya86/ImageNetDogs/">Link zum Datensatz</a></li>
            </ul>
        </section>

        <section>
            <h2>Datenvorverarbeitung - TODO</h2>
            <ul>
                <li>Train/Val-Split im Verhältnis 80%/20%</li>
                <li>Bildtransformationen für Trainingsdaten:
                    <ul>
                        <li>Resize auf 224x224px</li>
                        <li>Random Horizontal Flip</li>
                        <li>Random Rotation (10°)</li>
                        <li>Color Jitter (Helligkeit, Kontrast, Sättigung)</li>
                    </ul>
                </li>
                <li>Für Validierungsdaten: nur Resize und Normalisierung</li>
                <li>Normalisierung mit ImageNet-Mittelwerten: [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]</li>
            </ul>
        </section>

        <section>
            <h1>Modelltraining</h1>
        </section>

        <section>
            <h2>Modellauswahl</h2>
            <ul>
                <li>Verwendung eines Convolutional Neural Networks (CNN) für Bilddaten</li>
                <li>Begründung: CNNs sind spezialisiert auf die Extraktion von Merkmalen aus Bildern</li>
                <li>Transfer Learning mit ResNet50, vortrainiert auf ImageNet</li>
                <li>Fine-Tuning auf Corgis (Pembroke&Cardigan)</li>
                <li>Nur letzte Layer ersetzt: Dense Layer für 2-Klassen Klassifikation</li>
            </ul>
        </section>

        <section>
            <h2>Modellarchitektur & Training</h2>
            <ul>
                <li>Architektur: ResNet50 mit 50 Layern, Dense Layer für 2-Klassen Klassifikation</li>
                <li>Hyperparameter: Learning Rate 0.001, Batch Size 32</li>
                <li>Aktivierungsfunktionen: ReLU in den versteckten Schichten</li>
                <li>Training: CrossEntropyLoss als Verlustfunktion, Adam Optimizer</li>
                <li>Transfer Learning: Nur letzte Schichten trainiert</li>
                <li>10 Epochen für Feinabstimmung</li>
                <li>Aufteilung: 70% Training, 15% Validierung, 15% Test</li>
            </ul>
        </section>

        <section>
            <h2>Modell-Ergebnisse - TODO</h2>
            <ul>
                <li>Accuracy: >90% auf Validierungsdaten</li>
                <li>Transfer Learning mit Fine-Tuning der letzten Layer (layer4)</li>
                <li>Früher Trainingsabbruch durch Early Stopping (patience=5)</li>
                <li>Adaptive Lernrate mit ReduceLROnPlateau Scheduler</li>
                <li>Beste Ergebnisse bei Bildern mit klaren rassetypischen Merkmalen</li>
            </ul>
            <div>
                <img src="images/ExAI-Loss-and-Accuracy.png" alt="Loss and Accuracy" style="width:60.5%">
                <img src="images/ExAI-Confusion-Matrix.png" alt="Konfusionsmatrix" style="width:35%">
            </div>
        </section>

        <section>
            <h2>Erkennung von Mischlingen - TODO</h2>
            <ul>
                <li>Herausforderung: Mischlings-Corgis zeigen Merkmale beider Rassen</li>
                <li>Bei Bildern mit gemischten Charakteristika:
                    <ul>
                        <li>Niedrigere Konfidenz in der Vorhersage (meist <75%)</li>
                        <li>Aktivierungsmuster von GradCAM zeigen Überlappungen</li>
                        <li>LRP identifiziert widersprüchliche Merkmale beider Rassen</li>
                    </ul>
                </li>
                <li>XAI-Methoden ermöglichen Einblick in die "Unsicherheit" des Modells</li>
                <img src="./images/niko-royalty-free-image-1726720063.png" alt="Beispielbild_Corgi" style="width:40%">
            </ul>
        </section>

        <section>
            <h1>XAI Verfahren</h1>
        </section>

        <section>
            <h2>XAI-Verfahren</h2>
            <ol>
                <li><strong>Contrastive Grad-CAM</strong>: Visualisiert Unterschiede zwischen Klassen</li>
                <li><strong>Layerwise Relevance Propagation (LRP)</strong>: Liefert tiefere Einsicht auf Pixelebene</li>
            </ol>
            <p>Beide Methoden erlauben es, die Entscheidungen des Modells nachzuvollziehen</p>
        </section>

        <section>
            <h2>Grad-CAM: Technische Details</h2>
            <ul>
                <li>Verwendet Gradienten der letzten Convolutional Layer</li>
                <li>Berechnet gewichtete Aktivierungskarten</li>
                <li>Implementierung mit PyTorch Hooks für Forward/Backward Pass</li>
                <li>Target Layer: layer4 von ResNet50</li>
            </ul>
        </section>

        <section>
            <h2>Code-Snippet: Grad-CAM - TODO</h2>
            <pre><code class="language-python"># Grad-CAM Implementation
def generate(self, input_tensor, target_class=None):
    # Forward pass
    output = self.model(input_tensor)
    
    # Get target class if not specified
    if target_class is None:
        target_class = output.argmax(dim=1).item()
    
    # Zero gradients
    self.model.zero_grad()
    
    # Target for backprop
    one_hot = torch.zeros_like(output)
    one_hot[0, target_class] = 1
    
    # Backward pass
    output.backward(gradient=one_hot)
</code></pre>
        </section>

        <section>
            <h2>Visualisierung: Grad-CA - TODO</h2>
            <ul>
                <li>Hervorhebung entscheidungsrelevanter Bildregionen</li>
                <li>Bei Pembroke: Fokus auf Kopfform, Ohren und kurzen Schwanz</li>
                <li>Bei Cardigan: Fokus auf größere Ohren und längeren Schwanz</li>
                <li>Nutzung des letzten Convolutional Layers (layer4 von ResNet50)</li>
            </ul>
            <img src="images/gradcam_example.png" alt="Grad-CAM" style="width:70%">
            <p>Heatmap zeigt die für die Klassifikation relevanten Regionen</p>
        </section>

        <section>
            <h2>LRP: Technische Details</h2>
            <ul>
                <li>Propagiert Vorhersagen rückwärts durch das Netzwerk</li>
                <li>Berechnet Beiträge jedes Pixels zum finalen Output</li>
                <li>Relevanz-Regeln: Epsilon-Regel für Stabilität</li>
                <li>Implementierung für Conv2D, Linear und Pooling Layer</li>
            </ul>
        </section>

        <section>
            <h2>Code-Snippet: LRP - TODO</h2>
            <pre><code class="language-python"># LRP Implementation
def _lrp_conv(self, layer, input_tensor, output_tensor, relevance):
    # Get weights and reshape
    weights = layer.weight
    
    # Forward pass with epsilon stabilization
    z = torch.nn.functional.conv2d(input_tensor, weights, bias=layer.bias,
                                  stride=layer.stride, padding=layer.padding)
    z += self.epsilon * ((z >= 0).float() * 2 - 1)
    
    # Compute relevance contribution
    s = (relevance / z).data
    c = torch.nn.functional.conv_transpose2d(s, weights, stride=layer.stride,
                                          padding=layer.padding)
    relevance = input_tensor * c
    
    return relevance</code></pre>
        </section>

        <section>
            <h2>Visualisierung: LRP - TODO </h2>
            <ul>
                <li>Feinkörnige Visualisierung der Pixelrelevanz durch alle Netzwerkschichten hinweg</li>
                <li>Höhere Detailtiefe als GradCAM durch Berücksichtigung aller Layeraktivierungen</li>
                <li>Zeigt spezifische anatomische Details:
                    <ul>
                        <li>Bei Pembroke: Fokus auf Kopfform, Ohren und bestimmte Fellmuster</li>
                        <li>Bei Cardigan: Deutliche Aktivierung am Schwanz und den größeren Ohren</li>
                    </ul>
                </li>
                <li>Höherer Rechenaufwand, aber detailliertere Erklärung der Modellentscheidung</li>
            </ul>
            <img src="images/lrp_example.png" alt="LRP" style="width:70%">
            <p>Detaillierte Pixel-Relevanzverteilung zur finalen Entscheidung</p>
        </section>

        <section>
            <h1>Notebook</h1>
            <h4>Live-Demo</h4>
        </section>

        <section>
            <h1>Analyse & Kritische Diskussion</h1>
        </section>

<!--        <section>-->
<!--            <h2>Vergleich der Methoden</h2>-->
<!--            <img src="images/comparison.png" alt="Vergleich" style="width:80%">-->
<!--            <p>Links: Original, Mitte: Grad-CAM, Rechts: LRP</p>-->
<!--        </section>-->



        <section>
            <h2>Vergleich XAI-Methoden: Interpretierbarkeit</h2>
            <ul>
                <li><strong>Grad-CAM:</strong> Einfach zu verstehen, da visuelle Heatmaps erzeugt werden. Geeignet für Laien.</li>
                <li><strong>LRP:</strong> Liefert detaillierte Erklärungen auf Pixelebene, aber erfordert Expertenwissen zur Interpretation.</li>
            </ul>
        </section>

        <section>
            <h2>Vergleich XAI-Methoden: Modellabhängigkeit</h2>
            <ul>
                <li><strong>Grad-CAM:</strong> Funktioniert nur mit CNNs, da es Gradienten der Convolutional Layer nutzt.</li>
                <li><strong>LRP:</strong> Bedingt modellabhängig, kann auf verschiedene Architekturen angewendet werden, aber erfordert spezifische Anpassungen.</li>
            </ul>
        </section>

        <section>
            <h2>Vergleich XAI-Methoden: Genauigkeit & Konsistenz</h2>
            <ul>
                <li><strong>Grad-CAM:</strong> Liefert grobe Erklärungen, die für viele Inputs konsistent sind, aber weniger präzise.</li>
                <li><strong>LRP:</strong> Sehr präzise auf Pixelebene, aber die Ergebnisse können bei kleinen Änderungen im Input variieren.</li>
            </ul>
        </section>

        <section>
            <h2>Vergleich XAI-Methoden: Berechnungskosten</h2>
            <ul>
                <li><strong>Grad-CAM:</strong> Geringer Rechenaufwand, geeignet für große Datensätze und Echtzeitanwendungen.</li>
                <li><strong>LRP:</strong> Hoher Rechenaufwand, besonders bei tiefen Netzwerken und großen Datensätzen.</li>
            </ul>
        </section>

        <section>
            <h2>Vergleich XAI-Methoden: Anwendungsszenarien</h2>
            <ul>
                <li><strong>Grad-CAM:</strong> Ideal für schnelle visuelle Analysen, z. B. in der medizinischen Bildverarbeitung.</li>
                <li><strong>LRP:</strong> Geeignet für detaillierte Analysen, z. B. in der Forschung oder bei sicherheitskritischen Anwendungen.</li>
            </ul>
        </section>

        <section>
            <h2>Vergleich XAI-Methoden: Gesamtüberblick</h2>
            <table>
                <thead>
                <tr><th>Kriterium</th><th>Grad-CAM</th><th>LRP</th></tr>
                </thead>
                <tbody>
                <tr><td>Interpretierbarkeit</td><td>hoch</td><td>hoch</td></tr>
                <tr><td>Modellabhängigkeit</td><td>nur CNN</td><td>flexibel</td></tr>
                <tr><td>Genauigkeit</td><td>grob</td><td>fein</td></tr>
                <tr><td>Berechnungskosten</td><td>gering</td><td>hoch</td></tr>
                <tr><td>Anwendung</td><td>schnell</td><td>detailliert</td></tr>
                </tbody>
            </table>
        </section>

<!--        <section>-->
<!--            <h2>Contrastive Analysis</h2>-->
<!--            <ul>-->
<!--                <li>Vergleich der relevanten Merkmale für beide Klassen</li>-->
<!--                <li>Hervorhebung der unterscheidenden Features</li>-->
<!--                <li>Besonders wichtig bei ähnlichen Klassen wie Pembroke/Cardigan</li>-->
<!--            </ul>-->
<!--            <img src="images/contrastive_example.png" alt="Contrastive Analysis" style="width:60%">-->
<!--        </section>-->

        <section>
            <h2>Anwendungsfall: Mischling-Erkennung - TODO, Überfällig?</h2>
            <ul>
                <li>Bei Mischlingen zeigen beide Methoden überlappende Merkmale</li>
                <li>Unsicherheit des Modells korreliert mit Stärke der Merkmale</li>
                <li>XAI-Methoden zeigen Entscheidungsprozess transparent auf</li>
            </ul>
        </section>

        <section>
            <h2>Diskussion & Fazit</h2>
            <ul>
                <li>Modell liefert gute Ergebnisse für klar trennbare Klassen</li>
                <li>XAI-Methoden verdeutlichen die Entscheidungskriterien des Modells</li>
                <li>Grad-CAM: guter Überblick über relevante Regionen</li>
                <li>LRP: detailliertere Analyse auf Pixelebene</li>
<!--                <li>Kombination beider Methoden liefert umfassendes Verständnis</li>-->
            </ul>
        </section>

        <section>
            <h2>Ausblick</h2>
            <ul>
                <li>Integration weiterer XAI-Methoden (SHAP, Integrated Gradients)</li>
                <li>Erweiterung auf komplexere Klassifikationsaufgaben</li>
                <li>Quantitative Bewertung der XAI-Ergebnisse</li>
                <li>Verbesserung der Modellrobustheit durch XAI-Feedback</li>
            </ul>
        </section>

        <section>
            <h2>Vielen Dank!</h2>
            <h3>Fragen?</h3>
            <p><small>Projektteam: Lukas, Janik, Robin, Felix</small></p>
            <p><small>Code und Präsentation: <a href="https://github.com/mausio/ExAI">https://github.com/mausio/ExAI</a></small></p>
        </section>

    </div>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/reveal.min.js"></script>
<script src="dist/reveal.js"></script>
<script src="plugin/notes/notes.js"></script>
<script src="plugin/markdown/markdown.js"></script>
<script src="plugin/highlight/highlight.js"></script>
<script>
    Reveal.initialize({
        hash: true,
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
    });
</script>
</body>
</html>
