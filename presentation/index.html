<!doctype html>
<html lang="en">
<head>

    <meta charset="utf-8">
    <title>ExAI Projektpräsentation</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/reveal.min.css">
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/theme/black.min.css">
    <style>
        .reveal h1, .reveal h2, .reveal h3 {
            text-transform: none;
            color: #f0f0f0;
        }
        .reveal .slides {
            text-align: left;
        }
        .reveal img {
            margin: 15px 0;
            border-radius: 5px;
        }
        .reveal table {
            margin: 0 auto;
            border-collapse: collapse;
        }
        .reveal th, .reveal td {
            padding: 8px 15px;
            border: 1px solid #555;
        }
        .reveal pre code {
            padding: 15px;
            font-size: 0.8em;
        }
        .title-slide h1 {
            margin-top: 50px;
            font-size: 2.5em;
        }
        .title-slide h3 {
            margin-bottom: 50px;
            font-size: 1.3em;
            color: #ccc;
        }

        .reveal p {
            font-size: 0.7em;
        }

        .reveal li {
            font-size: 0.75em;
        }

        .reveal td {
            font-size: 0.9em;
        }

        .reveal th {
            font-size: 0.9em;
        }

        .reveal tr {
            font-size: 0.9em;
        }

        .reveal h4 {
            font-size: 1.0em;
        }

        .reveal h3 {
            font-size: 1.2em;
        }

        .reveal h2 {
            font-size: 1.3em;
        }

        .reveal h1 {
            font-size: 1.5em;
        }

    </style>
</head>
<body>
<div class="reveal">
    <div class="slides">

        <section class="title-slide">
            <h1>Explainable AI mit Bilddaten</h1>
            <h3>Unterscheidung von Pembroke und Cardigan Welsh Corgis</h3>
            <p>Lukas, Janik, Robin, Felix</p>
            <p><small>DHBW - ExAI Projekt</small></p>
        </section>

        <section>
            <h1>Agenda</h1>
            <ul>
                <li> Zielsetzung </li>
                <li> Datensatz-Auswahl & Preprocessing </li>
                <li> Modelltraining </li>
                <li> XAI Verfahren </li>
                <li> Notebook </li>
                <li> Analyse & Kritische Diskussion </li>
            </ul>
        </section>

        <section>
            <h1>Zielsetzung</h1>
        </section>

        <section>
            <h2>Problemstellung</h2>
            <p>Unterscheidung der Corgi-Rassen <strong>Pembroke</strong> und <strong>Cardigan</strong> mit Hilfe eines CNN.</p>
        </section>

        <section>
            <h2>Untersuchung</h2>
            <p>Beobachtung des Verhaltens bei Input von Mischlingen</p>
            <p>Ziel: Erklärbare Entscheidungen durch XAI-Methoden</p>
        </section>

        <section>
            <h1>Datensatz-Auswahl & Preprocessing</h1>
        </section>

        <section>
            <h2>Datensatz</h2>
            <ul>
                <li><strong>Stanford Dogs Dataset</strong></li>
                <li>120 Hunderassen, über 20.000 Bilder</li>
                <li>Verwendung der Klassen: Pembroke & Cardigan Welsh Corgi</li>
                <li><a href="http://vision.stanford.edu/aditya86/ImageNetDogs/">Link zum Datensatz</a></li>
            </ul>
        </section>

        <section>
            <h2>Datenvorverarbeitung</h2>
            <ul>
                <li>Train/Val-Split im Verhältnis 80%/20%</li>
                <li>Bildtransformationen für Trainingsdaten:
                    <ul>
                        <li>Resize auf 224x224px</li>
                        <li>Random Horizontal Flip</li>
                        <li>Random Rotation (10°)</li>
                        <li>Color Jitter (Helligkeit, Kontrast, Sättigung)</li>
                    </ul>
                </li>
                <li>Für Validierungsdaten: nur Resize und Normalisierung</li>
                <li>Normalisierung mit ImageNet-Mittelwerten: [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]</li>
            </ul>
        </section>

        <section>
            <h1>Modelltraining</h1>
        </section>

        <section>
            <h2>Modellauswahl</h2>
            <ul>
                <li>Verwendung eines Convolutional Neural Networks (CNN) für Bilddaten</li>
                <li>Begründung: CNNs sind spezialisiert auf die Extraktion von Merkmalen aus Bildern</li>
                <li>Transfer Learning mit ResNet50, vortrainiert auf ImageNet</li>
                <li>Fine-Tuning auf Corgis (Pembroke&Cardigan)</li>
                <li>Nur letzte Layer ersetzt: Dense Layer für 2-Klassen Klassifikation</li>
            </ul>
        </section>

        <section>
            <h2>Modellarchitektur & Training</h2>
            <ul>
                <li>Architektur: ResNet50 mit 50 Layern, Dense Layer für 2-Klassen Klassifikation</li>
                <li>Hyperparameter: Learning Rate 0.001, Batch Size 32</li>
                <li>Aktivierungsfunktionen: ReLU in den versteckten Schichten</li>
                <li>Training: CrossEntropyLoss als Verlustfunktion, Adam Optimizer</li>
                <li>Transfer Learning: Nur letzte Schichten trainiert</li>
                <li>10 Epochen für Feinabstimmung</li>
                <li>Aufteilung: 70% Training, 15% Validierung, 15% Test</li>
            </ul>
        </section>

        <section>
            <h2>Modell-Ergebnisse</h2>
            <ul>
                <li>Accuracy: >90% auf Validierungsdaten</li>
                <li>Transfer Learning mit Fine-Tuning der letzten Layer (layer4)</li>
                <li>Früher Trainingsabbruch durch Early Stopping (patience=5)</li>
                <li>Adaptive Lernrate mit ReduceLROnPlateau Scheduler</li>
                <li>Beste Ergebnisse bei Bildern mit klaren rassetypischen Merkmalen</li>
            </ul>
            <img src="images/ExAI-Loss-and-Accuracy.png" alt="Loss and Accuracy" style="width:60%; display:block; margin: 0 auto;"/>
            <p style="font-size: 0.8em; text-align: center;">
                Konstante Abnahme des Trainingsverlusts und Zunahme der Genauigkeit ohne Überanpassung.
                Die Validierungskurve stabilisiert sich bei etwa 93% Genauigkeit.
            </p>
        </section>

        <section>
            <h2>Modell-Ergebnisse: Confusion Matrix</h2>
            <img src="images/ExAI-Confusion-Matrix.png" alt="Konfusionsmatrix" style="width:40%; display:block; margin: 0 auto;"/>
            <p style="font-size: 0.8em; text-align: center;">
                Die Konfusionsmatrix zeigt: 38 korrekte Pembroke- und 24 korrekte Cardigan-Vorhersagen.
                Nur 6 Fehler insgesamt, hauptsächlich Cardigans als Pembrokes klassifiziert.
            </p>
        </section>

        <section>
            <h2>Ethische Betrachtungen</h2>
            <h3 style="font-size: 1em;">Ethik & Verantwortung</h3>
            <ul>
                <li>Transparenz als Voraussetzung für verantwortungsvolle KI-Systeme</li>
                <li>Vermeidung von Biases durch erklärbare Entscheidungsprozesse</li>
                <li>Datenschutz bei der Erfassung und Verarbeitung von Bilddaten</li>
                <li>Berücksichtigung ethischer Aspekte bei der Entwicklung von XAI-Methoden</li>
                <li>Förderung des Vertrauens in KI-Systeme durch transparente Entscheidungen</li>
                <li>Verantwortlicher Einsatz von KI in sensiblen Anwendungsbereichen</li>
            </ul>
        </section>

        <section>
            <h1>XAI Verfahren</h1>
        </section>

        <section>
            <h2>XAI-Verfahren im Überblick</h2>
            <ol>
                <li><strong>Contrastive Grad-CAM</strong>: Visualisiert Unterschiede zwischen Klassen</li>
                <li><strong>Layerwise Relevance Propagation (LRP)</strong>: Liefert tiefere Einsicht auf Pixelebene</li>
            </ol>
            <p>Beide Methoden erlauben es, die Entscheidungen des Modells nachzuvollziehen</p>
            <p>Direkte Vergleichsmöglichkeit der Erklärungsansätze bei verschiedenen Bildtypen (reinrassig vs. Mischlinge)</p>
        </section>

        <section>
            <h2>Grad-CAM: Technische Details</h2>
            <ul>
                <li>Verwendet Gradienten der letzten Convolutional Layer</li>
                <li>Berechnet gewichtete Aktivierungskarten</li>
                <li>Implementierung mit PyTorch Hooks für Forward/Backward Pass</li>
                <li>Target Layer: layer4 von ResNet50</li>
            </ul>
        </section>

        <section>
            <h2>Code-Snippet: Grad-CAM</h2>
            <h3>Teil 1: Forward Pass & Gradient-Berechnung</h3>
            <pre><code class="python" style="font-size: 0.7em; max-height: 350px;">
def __call__(self, input_tensor, target_class=None):
    # Forward pass
    input_tensor = input_tensor.to(device)
    self.model.zero_grad()
    
    # Forward pass through model
    output = self.model(input_tensor)
    
    # Use predicted class if none specified
    if target_class is None:
        target_class = torch.argmax(output, dim=1).item()
    
    # One-hot encoding for target class
    one_hot = torch.zeros_like(output)
    one_hot[0, target_class] = 1
    
    # Backward pass to get gradients
    output.backward(gradient=one_hot, retain_graph=True)
            </code></pre>
        </section>

        <section>
            <h2>Code-Snippet: Grad-CAM</h2>
            <h3>Teil 2: Aktivierungs-Gewichtung & Heatmap-Generierung</h3>
            <pre><code class="python" style="font-size: 0.7em; max-height: 350px;">
    # Weight the activations by gradients
    pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])
    for i in range(pooled_gradients.shape[0]):
        self.activations[:, i, :, :] *= pooled_gradients[i]
    
    # Average activations over channels
    cam = torch.mean(self.activations, dim=1).squeeze()
    
    # Apply ReLU and normalize
    cam = torch.maximum(cam, torch.tensor(0.0).to(device))
    if torch.max(cam) > 0:
        cam = cam / torch.max(cam)
        
    return cam.cpu().numpy()
            </code></pre>
        </section>

        <section>
            <h2>Visualisierung: Grad-CAM</h2> 
            <p>Heatmap zeigt die für die Klassifikation relevanten Regionen:</p>
            <div style="display: flex; align-items: center;">
                <img src="images/GradCAM-Beispiele.png" alt="Grad-CAM-Visual" style="width: 20%; margin-right: 20px; align-self: flex-start;" />
                    <ul style="font-size: 0.85em; align-self: flex-start;">
                        <li>Rote/gelbe Bereiche zeigen Hauptaufmerksamkeit des Modells</li>
                        <li>Bei Pembroke: Aktivierung im Kopf- und Halsbereich, typisch für die kürzere Statur</li>
                        <li>Bei Cardigan: Verstärkte Aktivierung im Schwanzbereich und größeren Ohren</li>
                        <li>Unterschiedliche Färbungsmuster beeinflussen Entscheidungsfindung</li>
                    </ul>
              </div>
        </section>

        <section>
            <h2>Visualisierung: Grad-CAM</h2>
            <ul>
                <li>Hervorhebung entscheidungsrelevanter Bildregionen</li>
                <li>Bei Pembroke: Fokus auf Kopfform, Ohren und kurzen Schwanz</li>
                <li>Bei Cardigan: Fokus auf größere Ohren und längeren Schwanz</li>
                <li>Nutzung des letzten Convolutional Layers (layer4 von ResNet50)</li>
            </ul>
        </section>

        <section>
            <h2>LRP: Technische Details</h2>
            <ul>
                <li>Propagiert Vorhersagen rückwärts durch das Netzwerk</li>
                <li>Berechnet Beiträge jedes Pixels zum finalen Output</li>
                <li>Relevanz-Regeln: Epsilon-Regel für Stabilität</li>
                <li>Implementierung für Conv2D, Linear und Pooling Layer</li>
            </ul>
        </section>

        <section>
            <h2>Code-Snippet: LRP</h2>
            <h3>Teil 1: Vorbereitung & Forward Pass</h3>
            <pre><code class="python" style="font-size: 0.7em; max-height: 350px;">
def __call__(self, input_tensor, target_class=None):
    # Clone input tensor and enable gradient tracking
    input_copy = input_tensor.clone().detach().to(device)
    input_copy.requires_grad = True
    
    # Forward pass
    self.model.zero_grad()
    output = self.model(input_copy)
    
    # Use predicted class if none specified
    if target_class is None:
        target_class = torch.argmax(output, dim=1).item()
    
    # Create one-hot encoding for target class
    one_hot = torch.zeros_like(output)
    one_hot[0, target_class] = 1.0
    
    # Backward pass to get gradients
    output.backward(gradient=one_hot)
            </code></pre>
        </section>

        <section>
            <h2>Code-Snippet: LRP</h2>
            <h3>Teil 2: Relevanz-Berechnung & Normalisierung</h3>
            <pre><code class="python" style="font-size: 0.7em; max-height: 350px;">
    # Get gradients with respect to input
    grad = input_copy.grad.clone()
    
    # Calculate relevance as element-wise product
    relevance = (input_copy * grad).sum(dim=1).squeeze()
    
    # Take absolute value and normalize
    relevance = torch.abs(relevance)
    if torch.max(relevance) > 0:
        relevance = relevance / torch.max(relevance)
    
    return relevance.detach().cpu().numpy()
            </code></pre>
        </section>

        <section>
            <h2>Visualisierung: LRP</h2>
            <div style="display: flex; align-items: center;">
                <img src="images/ExAI-LRP.png" alt="LRP-Visual" style="width: 20%; margin-right: 20px;" />
                <p>
                    Detaillierte Pixel-Relevanzverteilung zur finalen Entscheidung
                </p>
              </div>
        </section>

        <section>
            <h2>Vergleich der XAI-Methoden bei Rassenmerkmalen</h2>
            <div style="display: flex;">
                <div style="flex: 1;">
                    <h3 style="font-size: 0.9em;">Pembroke Welsh Corgi</h3>
                    <ul>
                        <li>Fokus auf fuchsartige Kopfform</li>
                        <li>Hervorhebung der aufrechten, spitzen Ohren</li>
                        <li>Aktivierung bei bestimmten Fellmustern</li>
                        <li>Kurzer oder fehlender Schwanz</li>
                    </ul>
                </div>
                <div style="flex: 1;">
                    <h3 style="font-size: 0.9em;">Cardigan Welsh Corgi</h3>
                    <ul>
                        <li>Deutliche Aktivierung am langen Schwanz</li>
                        <li>Hervorhebung der größeren, runderen Ohren</li>
                        <li>Fokus auf breiteren Körperbau</li>
                        <li>Activation bei dunkleren Fellfarben</li>
                    </ul>
                </div>
            </div>
        </section>

        <section>
            <h2>Anwendungsfall: Mischling-Erkennung</h2>
            <ul>
                <li>Experiment: Bewertung von Mischlings-Bildern beider Rassen</li>
                <li>Beobachtung: Konfidenz des Modells sinkt bei gemischten Merkmalen (oft unter 75%)</li>
                <li>GradCAM: aktiviert Regionen beider Rassen gleichzeitig</li>
                <li>LRP: zeigt konfliktäre Pixel-Aktivierungen für beide Klassen</li>
                <li>XAI ermöglicht transparenten Einblick in Modell-Unsicherheit</li>
                <li>Ermöglicht besseres Verständnis von Entscheidungsgrenzen im Modell</li>
            </ul>
        </section>

        <section>
            <h2>Anwendungsfall: Mischling-Erkennung (Beispiel)</h2>
            <div style="display: flex; align-items: center;">
                <img src="./images/niko-royalty-free-image-1726720063.png" alt="Beispielbild_Corgi" style="width:35%; margin-right: 15px;">
                <div>
                    <p style="font-size: 0.8em;">
                        <strong>Fallbeispiel Mischling:</strong> Die XAI-Visualisierungen zeigen gemischte Merkmale:
                    </p>
                    <ul style="font-size: 0.8em;">
                        <li>Pembroke-typische Züge: Fuchsartige Kopfform, aufrechte Ohren, helleres Fell</li>
                        <li>Cardigan-typische Züge: Breiterer Körperbau, längerer Schwanz</li>
                        <li>GradCAM zeigt aktivierte Zonen in beiden typischen Merkmalsbereichen</li>
                        <li>LRP offenbart widersprüchliche Pixelmuster, was die geringere Vorhersagekonfidenz erklärt</li>
                    </ul>
                </div>
            </div>
        </section>

        <section>
            <h2>Kritische Betrachtung der XAI-Methoden</h2>
            <ul>
                <li>XAI-Visualisierungen bieten Erklärungen, aber keine kausalen Zusammenhänge</li>
                <li>Subjektivität in der Interpretation der Visualisierungen</li>
                <li>GradCAM: Fokus auf letzte Layer könnte wichtige frühe Features übersehen</li>
                <li>LRP: Höhere Komplexität erschwert intuitive Interpretation</li>
                <li>Beide Methoden benötigen Expertenwissen zur vollständigen Ausschöpfung</li>
                <li>Balance zwischen Erklärbarkeit und technischer Tiefe ist herausfordernd</li>
            </ul>
        </section>

        <section>
            <h1>Analyse & Kritische Diskussion</h1>
        </section>

        <section>
            <h2>Stärken & Grenzen des Ansatzes</h2>
            <ul>
                <li><strong>Stärken:</strong>
                    <ul>
                        <li>Hohe Klassifikationsgenauigkeit (>90%)</li>
                        <li>Transparente Entscheidungsprozesse durch XAI</li>
                        <li>Effiziente Nutzung von Transfer Learning</li>
                    </ul>
                </li>
                <li><strong>Grenzen:</strong>
                    <ul>
                        <li>Eingeschränkte Generalisierbarkeit bei untypischen Bildaufnahmen</li>
                        <li>Abhängigkeit von der Qualität des Trainingsdatensatzes</li>
                        <li>Interpretationsaufwand bei XAI-Methoden</li>
                    </ul>
                </li>
            </ul>
        </section>

        <section>
            <h2>Zusammenfassung der XAI-Erkenntnisse</h2>
            <ul>
                <li>Beide XAI-Methoden zeigen, dass das Modell tatsächlich die rassetypischen Merkmale erkennt</li>
                <li>Rasseunterschiede werden primär anhand anatomischer Features erkannt:
                    <ul>
                        <li>Schwanz (lang vs. kurz/fehlend)</li>
                        <li>Ohren (groß/rund vs. spitz/aufrecht)</li>
                        <li>Körperbau (breiter vs. schlanker)</li>
                    </ul>
                </li>
                <li>Bei Mischlingen: XAI offenbart die "Unsicherheit" des Modells visuell</li>
                <li>Direkter Vergleich zeigt komplementäre Stärken der Methoden: GradCAM (Übersicht) und LRP (Detail)</li>
            </ul>
        </section>

        <section>
            <h2>Vergleich XAI-Methoden: Gesamtüberblick</h2>
            <table>
                <thead>
                <tr><th>Kriterium</th><th>Grad-CAM</th><th>LRP</th></tr>
                </thead>
                <tbody>
                <tr><td>Interpretierbarkeit</td><td>hoch</td><td>hoch</td></tr>
                <tr><td>Modellabhängigkeit</td><td>nur CNN</td><td>flexibel</td></tr>
                <tr><td>Genauigkeit</td><td>grob</td><td>fein</td></tr>
                <tr><td>Berechnungskosten</td><td>gering</td><td>hoch</td></tr>
                <tr><td>Anwendung</td><td>schnell</td><td>detailliert</td></tr>
                </tbody>
            </table>
        </section>

        <section>
            <h2>Vergleich XAI-Methoden: Ergebnisse</h2>
            <img src="images/ExAI-GradCAM_vs_LRP.png" alt="LRP-Visual" style="width:45%; display: block; margin: 0 auto;" />
            <p style="font-size: 0.8em; text-align: center;">
                Direkter Vergleich: GradCAM (links) zeigt grobe Fokusregionen, während LRP (rechts) 
                pixelgenaue Merkmalszuordnung ermöglicht.
            </p>
        </section>

        <section>
            <h1>Notebook</h1>
            <h4>Live-Demo</h4>
        </section>

        <section>
            <h2>Praktische Anwendungsfälle</h2>
            <ul>
                <li><strong>Tiermedizinische Diagnose:</strong> Identifikation von Anomalien in Tierbildern</li>
                <li><strong>Zuchtanalyse:</strong> Objektive Bewertung von Rassemerkmalen</li>
                <li><strong>Bildsuche:</strong> Verbesserung von Suchergebnissen durch merkmalsbasierte Ähnlichkeiten</li>
                <li><strong>Qualitätssicherung:</strong> Überprüfung der Modellentscheidungen in sicherheitskritischen Anwendungen</li>
                <li><strong>Bildungsbereich:</strong> Visuelle Darstellung von Merkmalsunterschieden für Lernzwecke</li>
            </ul>
        </section>

        <section>
            <h2>Ausblick</h2>
            <ul>
                <li>Integration weiterer XAI-Methoden (SHAP, Integrated Gradients)</li>
                <li>Erweiterung auf komplexere Klassifikationsaufgaben</li>
                <li>Quantitative Bewertung der XAI-Ergebnisse</li>
                <li>Verbesserung der Modellrobustheit durch XAI-Feedback</li>
            </ul>
        </section>

        <section>
            <h2>Vielen Dank!</h2>
            <h3>Fragen?</h3>
            <p><small>Projektteam: Lukas, Janik, Robin, Felix</small></p>
            <p><small>Code und Präsentation: <a href="https://github.com/mausio/ExAI">https://github.com/mausio/ExAI</a></small></p>
        </section>

    </div>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/reveal.min.js"></script>
<script src="dist/reveal.js"></script>
<script src="plugin/notes/notes.js"></script>
<script src="plugin/markdown/markdown.js"></script>
<script src="plugin/highlight/highlight.js"></script>
<script>
    Reveal.initialize({
        hash: true,
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
    });
</script>
</body>
</html>
