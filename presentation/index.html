<!doctype html>
<html lang="en">
<head>

    <meta charset="utf-8">
    <title>ExAI Projektpräsentation</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/reveal.min.css">
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/theme/black.min.css">
    <style>
        .reveal h1, .reveal h2, .reveal h3 {
            text-transform: none;
            color: #f0f0f0;
        }
        .reveal .slides {
            text-align: left;
        }
        .reveal img {
            margin: 15px 0;
            border-radius: 5px;
        }
        .reveal table {
            margin: 0 auto;
            border-collapse: collapse;
        }
        .reveal th, .reveal td {
            padding: 8px 15px;
            border: 1px solid #555;
        }
        .reveal pre code {
            padding: 15px;
            font-size: 0.8em;
        }
        .title-slide h1 {
            margin-top: 50px;
            font-size: 2.5em;
        }
        .title-slide h3 {
            margin-bottom: 50px;
            font-size: 1.3em;
            color: #ccc;
        }
    </style>
</head>
<body>
<div class="reveal">
    <div class="slides">

        <section class="title-slide">
            <h1>Explainable AI mit Bilddaten</h1>
            <h3>Unterscheidung von Pembroke und Cardigan Welsh Corgis</h3>
            <p>Lukas, Janik, Robin, Felix</p>
            <p><small>DHBW - ExAI Projekt</small></p>
        </section>

        <section>
            <h2>Problemstellung</h2>
            <p>Unterscheidung von <strong>Pembroke</strong> und <strong>Cardigan Corgis</strong> mit Hilfe eines CNN.</p>
        </section>

        <section>
            <h2>Untersuchung</h2>
            <p>Beobachtung des Verhaltens bei Input von Mischlingen</p>
            <p>Ziel: Erklärbare Entscheidungen durch XAI-Methoden</p>
        </section>



        <section>
            <h2>Datensatz</h2>
            <ul>
                <li><strong>Stanford Dogs Dataset</strong></li>
                <li>120 Hunderassen, über 20.000 Bilder</li>
                <li>Verwendung der Klassen: Pembroke & Cardigan Welsh Corgi</li>
                <li><a href="http://vision.stanford.edu/aditya86/ImageNetDogs/">Link zum Datensatz</a></li>
            </ul>
        </section>

        <section>
            <h2>Datenvorverarbeitung</h2>
            <ul>
                <li>Train/Test/Val-Split</li>
                <li>Resize auf 224x224px</li>
                <li>Normalisierung auf Imagenet-Mittelwerte</li>
            </ul>
        </section>

        <section>
            <h2>Modell</h2>
            <ul>
                <li>Transfer Learning mit ResNet50</li>
                <li>Pretrained auf ImageNet, Fine-Tuning auf Corgis (Pembroke&Cardigan)</li>
                <li>Nur letzte Layer ersetzt: Dense Layer für 2-Klassen Klassifikation</li>
            </ul>
        </section>

        <section>
            <h2>Modelltraining</h2>
            <ul>
                <li>CrossEntropyLoss als Verlustfunktion</li>
                <li>Adam Optimizer mit Learning Rate 0.001</li>
                <li>Transfer Learning: Nur letzte Schichten trainiert</li>
                <li>10 Epochen für Feinabstimmung</li>
                <li>Aufteilung: 70% Training, 15% Validierung, 15% Test</li>
            </ul>
        </section>

        <section>
            <h2>Modell-Ergebnisse</h2>
            <ul>
                <li>Accuracy: ~92% auf Testdaten</li>
                <li>Fehler hauptsächlich bei Bildern mit untypischen Merkmalen</li>
                <li>Konfusionsmatrix zeigt gute Trennung der Klassen</li>
            </ul>
            <img src="images/confusion_matrix.png" alt="Konfusionsmatrix" style="width:40%">
        </section>

        <section>
            <h2>Erkennung von Mischlingen</h2>
            <ul>
                <li>Beispielbilder zeigen überlappende Merkmale von beiden Corgi-Typen</li>
                <li>Modell "unsicher" → Indiz für fehlende Klarheit der Merkmale</li>
            </ul>
        </section>

        <section>
            <h2>XAI-Verfahren</h2>
            <ol>
                <li><strong>Contrastive Grad-CAM</strong>: Visualisiert Unterschiede zwischen Klassen</li>
                <li><strong>Layerwise Relevance Propagation (LRP)</strong>: Liefert tiefere Einsicht auf Pixelebene</li>
            </ol>
            <p>Beide Methoden erlauben es, die Entscheidungen des Modells nachzuvollziehen</p>
        </section>

        <section>
            <h2>Grad-CAM: Technische Details</h2>
            <ul>
                <li>Verwendet Gradienten der letzten Convolutional Layer</li>
                <li>Berechnet gewichtete Aktivierungskarten</li>
                <li>Implementierung mit PyTorch Hooks für Forward/Backward Pass</li>
                <li>Target Layer: layer4 von ResNet50</li>
            </ul>
        </section>

        <section>
            <h2>Code-Snippet: Grad-CAM</h2>
            <pre><code class="language-python"># Grad-CAM Implementation
def generate(self, input_tensor, target_class=None):
    # Forward pass
    output = self.model(input_tensor)
    
    # Get target class if not specified
    if target_class is None:
        target_class = output.argmax(dim=1).item()
    
    # Zero gradients
    self.model.zero_grad()
    
    # Target for backprop
    one_hot = torch.zeros_like(output)
    one_hot[0, target_class] = 1
    
    # Backward pass
    output.backward(gradient=one_hot)
</code></pre>
        </section>

        <section>
            <h2>Visualisierung: Grad-CAM</h2>
            <img src="images/gradcam_example.png" alt="Grad-CAM" style="width:70%">
            <p>Hervorgehobene Regionen für Entscheidung "Pembroke"</p>
        </section>

        <section>
            <h2>LRP: Technische Details</h2>
            <ul>
                <li>Propagiert Vorhersagen rückwärts durch das Netzwerk</li>
                <li>Berechnet Beiträge jedes Pixels zum finalen Output</li>
                <li>Relevanz-Regeln: Epsilon-Regel für Stabilität</li>
                <li>Implementierung für Conv2D, Linear und Pooling Layer</li>
            </ul>
        </section>

        <section>
            <h2>Code-Snippet: LRP</h2>
            <pre><code class="language-python"># LRP Implementation
def _lrp_conv(self, layer, input_tensor, output_tensor, relevance):
    # Get weights and reshape
    weights = layer.weight
    
    # Forward pass with epsilon stabilization
    z = torch.nn.functional.conv2d(input_tensor, weights, bias=layer.bias,
                                  stride=layer.stride, padding=layer.padding)
    z += self.epsilon * ((z >= 0).float() * 2 - 1)
    
    # Compute relevance contribution
    s = (relevance / z).data
    c = torch.nn.functional.conv_transpose2d(s, weights, stride=layer.stride,
                                          padding=layer.padding)
    relevance = input_tensor * c
    
    return relevance</code></pre>
        </section>

        <section>
            <h2>Visualisierung: LRP</h2>
            <img src="images/lrp_example.png" alt="LRP" style="width:70%">
            <p>Pixel-Relevanzen zur finalen Entscheidung</p>
        </section>

        <section>
            <h2>Vergleich der Methoden</h2>
            <img src="images/comparison.png" alt="Vergleich" style="width:80%">
            <p>Links: Original, Mitte: Grad-CAM, Rechts: LRP</p>
        </section>

        <section>
            <h2>Vergleich XAI-Methoden</h2>
            <table>
                <thead>
                <tr><th>Kriterium</th><th>Grad-CAM</th><th>LRP</th></tr>
                </thead>
                <tbody>
                <tr><td>Interpretierbarkeit</td><td>hoch</td><td>hoch</td></tr>
                <tr><td>Modellabhängigkeit</td><td>nur CNN</td><td>bedingt modellabhängig</td></tr>
                <tr><td>Rechenaufwand</td><td>gering</td><td>hoch</td></tr>
                <tr><td>Auflösung</td><td>grob</td><td>fein</td></tr>
                </tbody>
            </table>
        </section>

        <section>
            <h2>Contrastive Analysis</h2>
            <ul>
                <li>Vergleich der relevanten Merkmale für beide Klassen</li>
                <li>Hervorhebung der unterscheidenden Features</li>
                <li>Besonders wichtig bei ähnlichen Klassen wie Pembroke/Cardigan</li>
            </ul>
            <img src="images/contrastive_example.png" alt="Contrastive Analysis" style="width:60%">
        </section>

        <section>
            <h2>Anwendungsfall: Mischling-Erkennung</h2>
            <ul>
                <li>Bei Mischlingen zeigen beide Methoden überlappende Merkmale</li>
                <li>Unsicherheit des Modells korreliert mit Stärke der Merkmale</li>
                <li>XAI-Methoden zeigen Entscheidungsprozess transparent auf</li>
            </ul>
        </section>

        <section>
            <h2>Diskussion & Fazit</h2>
            <ul>
                <li>Modell liefert gute Ergebnisse für klar trennbare Klassen</li>
                <li>XAI-Methoden verdeutlichen die Entscheidungskriterien des Modells</li>
                <li>Grad-CAM: guter Überblick über relevante Regionen</li>
                <li>LRP: detailliertere Analyse auf Pixelebene</li>
                <li>Kombination beider Methoden liefert umfassendes Verständnis</li>
            </ul>
        </section>

        <section>
            <h2>Ausblick</h2>
            <ul>
                <li>Integration weiterer XAI-Methoden (SHAP, Integrated Gradients)</li>
                <li>Erweiterung auf komplexere Klassifikationsaufgaben</li>
                <li>Quantitative Bewertung der XAI-Ergebnisse</li>
                <li>Verbesserung der Modellrobustheit durch XAI-Feedback</li>
            </ul>
        </section>

        <section>
            <h2>Vielen Dank!</h2>
            <h3>Fragen?</h3>
            <p><small>Projektteam: Lukas, Janik, Robin, Felix</small></p>
            <p><small>Code und Präsentation: <a href="https://github.com/username/exai-corgis">github.com/username/exai-corgis</a></small></p>
        </section>

    </div>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/reveal.min.js"></script>
<script src="dist/reveal.js"></script>
<script src="plugin/notes/notes.js"></script>
<script src="plugin/markdown/markdown.js"></script>
<script src="plugin/highlight/highlight.js"></script>
<script>
    Reveal.initialize({
        hash: true,
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
    });
</script>
</body>
</html>
