{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "GSjCu3qxLzNK",
   "metadata": {
    "id": "GSjCu3qxLzNK"
   },
   "source": [
    "# ExAI - Explainable Corgi (Cardigan) Separator ðŸ¶\n",
    "\n",
    "We use [Contrastive GradCAM](https://xai-blog.netlify.app/docs/groups/contrastive-grad-cam-consistency/#contrastive-grad-cam-consistency-loss)\n",
    "and [Layerwise Relevance Propagation](https://github.com/kaifishr/PyTorchRelevancePropagation) to explain the difference between Corgis and Cardigans - two breeds that are often difficult to distinguish visually even for dog experts.\n",
    "\n",
    "Key visual differences include tail length (Cardigans have longer tails), ear size (Cardigans have larger ears), and body length (Cardigans typically have longer bodies). Our XAI techniques aim to determine if these are indeed the features our model focuses on when making classifications.\n",
    "\n",
    "- We leverage [Stanford ImageNet Dog Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) for fine-tuning [ResNet50](https://pytorch.org/hub/pytorch_vision_resnet/#model-description).\n",
    "- Target breeds: [Pembroke Welsh Corgi](https://de.wikipedia.org/wiki/Welsh_Corgi_Pembroke) | [Cardigan Welsh Corgi](https://de.wikipedia.org/wiki/Welsh_Corgi_Cardigan)\n",
    "\n",
    "## The Process:\n",
    "1. Load the dataset and split it into training (80%) and validation (20%) sets.\n",
    "2. Fine-tune a pre-trained ResNet50 model using transfer learning (freezing early layers).\n",
    "3. Evaluate model performance through accuracy metrics and confusion matrix.\n",
    "4. Apply two XAI techniques to visualize decision factors:\n",
    "   - **GradCAM**: Highlights regions that most influenced the class prediction\n",
    "   - **LRP**: Provides pixel-level relevance scores for the entire image\n",
    "5. Compare both techniques to understand if the model focuses on breed-specific anatomical features.\n",
    "6. Analyze whether our model's reasoning aligns with established breed characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dJH_W2oaOJoB",
   "metadata": {
    "id": "dJH_W2oaOJoB"
   },
   "source": [
    "## 1. Data/Dependency Loading and Transformation\n",
    "\n",
    "This section covers the foundational setup for our Corgi classification pipeline, including:\n",
    "- Import of essential libraries and dependencies\n",
    "- Data acquisition from Stanford Dogs Dataset\n",
    "- Custom dataset class implementation for Corgi breeds\n",
    "- Data augmentation and preprocessing transformations\n",
    "- Creation of training and validation data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11000ff2",
   "metadata": {},
   "source": [
    "### Import of all necessary packages/libraries \n",
    "\n",
    "We import PyTorch and related libraries for deep learning, along with NumPy, Matplotlib, and other data processing tools. These packages enable us to build our classification pipeline, handle image data, train our CNN model, and visualize the XAI results for comparing Pembroke and Cardigan Welsh Corgis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bPBrZtQvR0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6bPBrZtQvR0",
    "is_executing": true,
    "outputId": "6501edec-c30f-4a80-b3ce-128ff28b9c00"
   },
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import os\n",
    "import time #time measurement and delays \n",
    "import copy \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import seaborn as sns # statistical data visualization\n",
    "from tqdm import tqdm # progress bars for loops\n",
    "from PIL import Image # image loading\n",
    "import cv2 #image processing \n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn # neural network layers\n",
    "import torch.optim as optim # optimization algorithms\n",
    "from torch.optim import lr_scheduler # learning rate scheduling\n",
    "from torch.utils.data import Dataset, DataLoader, random_split # dataset handling\n",
    "import torchvision # for image transformations\n",
    "from torchvision import transforms, models # pre-trained models\n",
    "from torchvision.datasets.utils import download_url, extract_archive # downloading datasets\n",
    "\n",
    "# For evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report # for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932de5c0",
   "metadata": {},
   "source": [
    "### Setting device for GPU acceleration\n",
    "\n",
    "We configure PyTorch to utilize available GPU resources through CUDA, significantly accelerating the training process and matrix operations. If no GPU is available, the code automatically falls back to CPU processing, ensuring compatibility across different hardware environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "SJYyGFcNMcRI",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "SJYyGFcNMcRI",
    "outputId": "2d1015e3-417b-4d9d-a47e-54b95ffc60f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounting Google Drive...\n",
      "/\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "-= Done =-\n"
     ]
    }
   ],
   "source": [
    "# Setup device for training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018e2423",
   "metadata": {},
   "source": [
    "### Downloading Dataset if not already in directory\n",
    "[Stanford Dogs Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/):\n",
    "\n",
    "> The Stanford Dogs dataset contains images of 120 breeds of dogs from around the world. This dataset has been built using images and annotation from ImageNet for the task of fine-grained image categorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "initial_id",
    "outputId": "6c3b9c7f-2926-4caf-e2a3-84370e60329b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded images.tar to /content/drive/MyDrive/xAI-Corgis/images.tar\n"
     ]
    }
   ],
   "source": [
    "def download_and_extract_dataset(download_dir, extract_dir):\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "    \n",
    "    # Download the dataset\n",
    "    dataset_url = \"http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\"\n",
    "    filename = os.path.basename(dataset_url)\n",
    "    filepath = os.path.join(download_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        download_url(dataset_url, download_dir)\n",
    "    else:\n",
    "        print(f\"File {filename} already exists in {download_dir}\")\n",
    "    \n",
    "    # Extract the dataset\n",
    "    if not os.path.exists(os.path.join(extract_dir, \"Images\")):\n",
    "        print(f\"Extracting {filename} to {extract_dir}...\")\n",
    "        extract_archive(filepath, extract_dir)\n",
    "    else:\n",
    "        print(f\"Dataset already extracted to {extract_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84b374a",
   "metadata": {},
   "source": [
    "### Corgi Dataset Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1af7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CorgiDataset(Dataset):\n",
    "    def __init__(self, dataset_root, transform=None):\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_names = ['Pembroke', 'Cardigan']\n",
    "        \n",
    "        images_dir = os.path.join(dataset_root, \"Images\")\n",
    "        if not os.path.exists(images_dir):\n",
    "            raise FileNotFoundError(f\"Images directory not found at {images_dir}\")\n",
    "            \n",
    "        all_breeds = os.listdir(images_dir)\n",
    "        \n",
    "        pembroke_dir = None\n",
    "        cardigan_dir = None\n",
    "        \n",
    "        for breed in all_breeds:\n",
    "            if \"Pembroke\" in breed:\n",
    "                pembroke_dir = os.path.join(images_dir, breed)\n",
    "            elif \"Cardigan\" in breed:\n",
    "                cardigan_dir = os.path.join(images_dir, breed)\n",
    "        \n",
    "        if not pembroke_dir or not cardigan_dir:\n",
    "            raise ValueError(\"Could not find Pembroke or Cardigan directories\")\n",
    "        \n",
    "        print(f\"Pembroke directory: {pembroke_dir}\")\n",
    "        print(f\"Cardigan directory: {cardigan_dir}\")\n",
    "        \n",
    "        for img_name in os.listdir(pembroke_dir):\n",
    "            if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                self.image_paths.append(os.path.join(pembroke_dir, img_name))\n",
    "                self.labels.append(0)  # Pembroke\n",
    "        \n",
    "        for img_name in os.listdir(cardigan_dir):\n",
    "            if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                self.image_paths.append(os.path.join(cardigan_dir, img_name))\n",
    "                self.labels.append(1)  # Cardigan\n",
    "        \n",
    "        print(f\"Total number of images: {len(self.image_paths)}\")\n",
    "        print(f\"Pembroke images: {sum(1 for label in self.labels if label == 0)}\")\n",
    "        print(f\"Cardigan images: {sum(1 for label in self.labels if label == 1)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            label = self.labels[idx]\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a blank image and the same label\n",
    "            blank_image = torch.zeros((3, 224, 224)) if self.transform else Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "            return blank_image, self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a38e9",
   "metadata": {},
   "source": [
    "### Class for transforming Subset\n",
    "\n",
    "When splitting data into training and validation sets, we get Subset objects that don't directly support transformations. \n",
    "\n",
    "TransformedSubset enables us to apply different data strategies to training data versus validation data, which is essential to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48J5ZvoSk-Om",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48J5ZvoSk-Om",
    "outputId": "558113be-691d-477d-b756-670b7e5bc477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.tar successfully extracted to: '/content/dogs'.\n"
     ]
    }
   ],
   "source": [
    "class TransformedSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Correctly handle the subset indexing\n",
    "        image, label = self.subset.dataset[self.subset.indices[idx]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783f4d6",
   "metadata": {},
   "source": [
    "### Data Preparation and Loaders\n",
    "\n",
    "This section creates our data pipeline for efficient model training. It has different functions:\n",
    "1. **Data Transformations**: Defines separate strategies for training.\n",
    "2. **Dataset Splitting**: Creates an 80/20 train/validation split.\n",
    "3. **Optimized Loading**: Configures DataLoaders with batch processing, etc.\n",
    "\n",
    "This approach ensures our model trains on varied examples while being evaluated on consistent, unmodified validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fba4851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(dataset_root, batch_size=32, num_workers=2):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    dataset = CorgiDataset(dataset_root, transform=None)  # No transform yet\n",
    "    \n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_dataset_transformed = TransformedSubset(train_dataset, data_transforms['train'])\n",
    "    val_dataset_transformed = TransformedSubset(val_dataset, data_transforms['val'])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset_transformed, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset_transformed, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {len(train_dataset)} images\")\n",
    "    print(f\"Validation set size: {len(val_dataset)} images\")\n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    return train_loader, val_loader, dataset.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s37ot4N9OQGQ",
   "metadata": {
    "id": "s37ot4N9OQGQ"
   },
   "source": [
    "## 2. Model Definition\n",
    "\n",
    "This section implements the transfer learning approach using ResNet50:\n",
    "\n",
    "1. **Pre-trained Model Loading**: We load ResNet50 with weights pre-trained on ImageNet\n",
    "2. **Layer Freezing Strategy**: We employ a strategic freezing pattern.\n",
    "3. **Custom Classification Head**: The original fully-connected layer is replaced with ReLU (Rectified Linear Unit)\n",
    "\n",
    "This approach dramatically reduces training time and required data while maintaining high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7441e4",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X71aR-0Y5O-7",
   "metadata": {
    "cellView": "form",
    "id": "X71aR-0Y5O-7"
   },
   "outputs": [],
   "source": [
    "\n",
    "def setup_model(num_classes=2):\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for param in model.layer4.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"ResNet50 model configured for {num_classes} classes\")\n",
    "    print(f\"Trainable parameters in model.layer4: {sum(p.numel() for p in model.layer4.parameters() if p.requires_grad)}\")\n",
    "    print(f\"Trainable parameters in model.fc: {sum(p.numel() for p in model.fc.parameters() if p.requires_grad)}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cef616",
   "metadata": {},
   "source": [
    "### Training Functions\n",
    "\n",
    "This section contains the core training pipeline components that manage the model training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b4b0b",
   "metadata": {},
   "source": [
    "#### Training Function: Epoch\n",
    "\n",
    "This function handles a single training iteration through the entire dataset. For each batch of images:\n",
    "1. It transfers data to GPU/CPU and zeroes gradients\n",
    "2. Performs predictions \n",
    "3. Calculates loss using cross-entropy (loss function)\n",
    "4. Executes backward propagation to compute gradients\n",
    "5. Updates model weights via optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o-2VF4oKDqRw",
   "metadata": {
    "id": "o-2VF4oKDqRw"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "    \n",
    "    print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    \n",
    "    return epoch_loss, epoch_acc.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f1783f",
   "metadata": {},
   "source": [
    "#### Training Function: Validate Epoch\n",
    "\n",
    "Evaluates model performance on validation data without updating weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TssTfUXeH_C_",
   "metadata": {
    "id": "TssTfUXeH_C_"
   },
   "outputs": [],
   "source": [
    "\n",
    "def validate_epoch(model, dataloader, criterion):\n",
    "    model.eval()  # Set model to evaluate mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    # Iterate over data\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "    \n",
    "    print(f'Validation Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    \n",
    "    return epoch_loss, epoch_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d3918",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "This orchestration function manages the entire training cycle. It implements:\n",
    "\n",
    "1. **Setting Up the Learning Process**: We use CrossEntropy and configure different learning speeds for different parts of the network\n",
    "2. **Adaptive Learning Rate**: our scheduler reduces learning rates when improvement slows down\n",
    "3. **Training Loop**: The main training cycle runs through our data multiple times (epochs), tracking both how well we're memorizing training data and how well we generalize to new images \n",
    "4. **Smart Quitting**: We stop training when validation accuracy doesn't improve for 5 consecutive epochs - preventing the model from just memorizing training examples\n",
    "5. **Saving the Best Version**: Rather than keeping the final model, we save a copy of the model weights whenever it achieves a new high score on validation data\n",
    "\n",
    "This approach maximizes efficiency by preventing overfitting while ensuring the model reaches optimal performance for our Corgi classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=15, patience=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.layer4.parameters(), 'lr': 1e-4},\n",
    "        {'params': model.fc.parameters(), 'lr': 1e-3}\n",
    "    ], weight_decay=1e-5)\n",
    "    \n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "    \n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    no_improve_epochs = 0\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 40)\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "        \n",
    "        print(f'Best val Acc: {best_acc:.4f}')\n",
    "        \n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement')\n",
    "            break\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e3395",
   "metadata": {},
   "source": [
    "### Evaluating Model\n",
    "\n",
    "This function tests how well our model recognizes different Corgis:\n",
    "\n",
    "1. It runs validation images through the model and compares predictions to actual labels\n",
    "2. Creates a colorful grid (confusion matrix) showing correct guesses vs. mistakes\n",
    "3. Calculates accuracy scores for each breed\n",
    "4. Returns all results for analysis\n",
    "\n",
    "Basically, it's like giving our model a final exam and creating a detailed report card!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52031ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, dataloader, class_names):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    return y_true, y_pred, report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d22702",
   "metadata": {},
   "source": [
    "### Plotting the History\n",
    "\n",
    "This function visualizes two key metrics:\n",
    "1. **Loss trends** - Shows how the error decreases during training\n",
    "2. **Accuracy trends** - Shows how prediction accuracy improves\n",
    "\n",
    "Both metrics are plotted for training and validation data, helping identify when the model starts overfitting (when validation metrics worsen while training metrics continue to improve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c1883",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d71d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(model, save_path, class_names, optimizer=None, epoch=None, history=None):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    torch.save(model, save_path.replace('.pth', '_full.pth'))\n",
    "    print(f\"Complete model saved to: {save_path.replace('.pth', '_full.pth')}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), save_path.replace('.pth', '_weights.pth'))\n",
    "    print(f\"Model weights saved to: {save_path.replace('.pth', '_weights.pth')}\")\n",
    "    \n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'classes': class_names,\n",
    "    }\n",
    "    \n",
    "    if optimizer:\n",
    "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
    "    \n",
    "    if epoch is not None:\n",
    "        checkpoint['epoch'] = epoch\n",
    "        \n",
    "    if history:\n",
    "        checkpoint['history'] = history\n",
    "    \n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Checkpoint saved to: {save_path}\")\n",
    "    \n",
    "    try:\n",
    "        dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            save_path.replace('.pth', '.onnx'),\n",
    "            export_params=True,\n",
    "            opset_version=11,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "        )\n",
    "        print(f\"ONNX model saved to: {save_path.replace('.pth', '.onnx')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting to ONNX format: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb0f01",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c4808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(load_path, model=None):\n",
    "    \"\"\"\n",
    "    Loads a model from a file\n",
    "    \n",
    "    Args:\n",
    "        load_path: Path to load the model from\n",
    "        model: Model to load weights into (optional)\n",
    "        \n",
    "    Returns:\n",
    "        model: The loaded model\n",
    "        checkpoint: The loaded checkpoint\n",
    "    \"\"\"\n",
    "    try:\n",
    "        checkpoint = torch.load(load_path, map_location=device)\n",
    "        \n",
    "        if model is None:\n",
    "            # Try loading the full model\n",
    "            if load_path.endswith('_full.pth'):\n",
    "                model = torch.load(load_path, map_location=device)\n",
    "                print(f\"Full model loaded from: {load_path}\")\n",
    "                return model, None\n",
    "            \n",
    "            # Otherwise create a new model\n",
    "            model = setup_model()\n",
    "        \n",
    "        # Load state dict if it exists\n",
    "        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        \n",
    "        print(f\"Model weights loaded from: {load_path}\")\n",
    "        return model, checkpoint\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f6bdd",
   "metadata": {},
   "source": [
    "## 3. Model Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acc97f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    download_dir = \"/content/drive/MyDrive/xAI-Corgis\" # TODO: Change to local directory\n",
    "    extract_dir = \"/content/dogs\" # TODO: Change to local directory \n",
    "    \n",
    "    download_and_extract_dataset(download_dir, extract_dir)\n",
    "    \n",
    "    train_loader, val_loader, class_names = prepare_dataloaders(extract_dir, batch_size=32)\n",
    "    \n",
    "    model = setup_model(num_classes=len(class_names))\n",
    "    \n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    model_path = os.path.join(download_dir, 'resnet50_corgi_classifier.pth')\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading pre-trained model...\")\n",
    "        model, checkpoint = load_model(model_path)\n",
    "    else:\n",
    "        print(\"Training a new model...\")\n",
    "        model, history = train_model(model, train_loader, val_loader, num_epochs=15)\n",
    "        \n",
    "        plot_training_history(history)\n",
    "        \n",
    "        y_true, y_pred, report = evaluate_model(model, val_loader, class_names)\n",
    "        \n",
    "        save_model(\n",
    "            model, \n",
    "            model_path,\n",
    "            class_names=class_names,\n",
    "            history=history\n",
    "        )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Applying XAI Methods for Model Interpretability\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nGenerating GradCAM visualizations...\")\n",
    "    visualize_gradcam(model, val_loader, class_names, num_images=5)\n",
    "    \n",
    "    print(\"\\nGenerating Layer-wise Relevance Propagation visualizations...\")\n",
    "    visualize_lrp(model, val_loader, class_names, num_images=5)\n",
    "    \n",
    "    print(\"\\nComparing GradCAM and LRP methods...\")\n",
    "    compare_xai_methods(model, val_loader, class_names, num_images=3)\n",
    "    \n",
    "    print(\"\\nXAI visualization complete. All results saved as PNG files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ad237",
   "metadata": {},
   "source": [
    "## 4. xAI Methods \n",
    "\n",
    "Talk about the xAI methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8def829",
   "metadata": {},
   "source": [
    "### GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530639fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.hooks = []\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.register_hooks()\n",
    "        self.model.eval()\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "            \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0].detach()\n",
    "            \n",
    "        forward_handle = self.target_layer.register_forward_hook(forward_hook)\n",
    "        backward_handle = self.target_layer.register_full_backward_hook(backward_hook)\n",
    "        \n",
    "        self.hooks = [forward_handle, backward_handle]\n",
    "        \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Removes all registered hooks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "            \n",
    "    def __call__(self, input_tensor, target_class=None):\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        output = self.model(input_tensor)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(output, dim=1).item()\n",
    "        \n",
    "        one_hot = torch.zeros_like(output)\n",
    "        one_hot[0, target_class] = 1\n",
    "        \n",
    "        output.backward(gradient=one_hot, retain_graph=True)\n",
    "        \n",
    "        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])\n",
    "        \n",
    "        for i in range(pooled_gradients.shape[0]):\n",
    "            self.activations[:, i, :, :] *= pooled_gradients[i]\n",
    "        \n",
    "        cam = torch.mean(self.activations, dim=1).squeeze()\n",
    "        \n",
    "        cam = torch.maximum(cam, torch.tensor(0.0).to(device))\n",
    "        \n",
    "        if torch.max(cam) > 0:\n",
    "            cam = cam / torch.max(cam)\n",
    "        \n",
    "        cam = cam.cpu().numpy()\n",
    "        \n",
    "        return cam\n",
    "\n",
    "def apply_gradcam(model, img_tensor, img_np, target_class=None, layer_name='layer4'):\n",
    "    target_layer = model.layer4\n",
    "    \n",
    "    grad_cam = GradCAM(model, target_layer)\n",
    "    \n",
    "    cam = grad_cam(img_tensor, target_class)\n",
    "    \n",
    "    cam_resized = cv2.resize(cam, (img_np.shape[1], img_np.shape[0]))\n",
    "    \n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
    "    \n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    alpha = 0.4\n",
    "    visualization = heatmap * alpha + img_np * (1 - alpha)\n",
    "    visualization = np.uint8(visualization)\n",
    "    \n",
    "    grad_cam.remove_hooks()\n",
    "    \n",
    "    return visualization, cam\n",
    "\n",
    "def visualize_gradcam(model, dataloader, class_names, num_images=5):\n",
    "    model.eval()\n",
    "    \n",
    "    images, labels = next(iter(dataloader))\n",
    "    images = images[:num_images]\n",
    "    labels = labels[:num_images]\n",
    "    \n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))\n",
    "    \n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        img_np = image.cpu().numpy().transpose(1, 2, 0)\n",
    "        img_np = np.clip(img_np * np.array([0.229, 0.224, 0.225]) + \n",
    "                        np.array([0.485, 0.456, 0.406]), 0, 1)\n",
    "        \n",
    "        input_tensor = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            prob = torch.nn.functional.softmax(output, dim=1)\n",
    "        \n",
    "        true_cam, _ = apply_gradcam(model, input_tensor, img_np, label.item())\n",
    "        \n",
    "        pred_cam, _ = apply_gradcam(model, input_tensor, img_np, pred.item())\n",
    "        \n",
    "        axes[i, 0].imshow(img_np)\n",
    "        axes[i, 0].set_title(f\"True: {class_names[label]}\\nPred: {class_names[pred]} ({prob[0][pred.item()]:.2f})\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(true_cam)\n",
    "        axes[i, 1].set_title(f\"GradCAM for {class_names[label]}\")\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(pred_cam)\n",
    "        axes[i, 2].set_title(f\"GradCAM for {class_names[pred]}\")\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('gradcam_visualizations.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa647935",
   "metadata": {},
   "source": [
    "### Layer-wise Relevance Propagation (LRP) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LRP:\n",
    "    def __init__(self, model, epsilon=1e-9):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.model.eval()\n",
    "        \n",
    "    def _clone_module(self, module, memo=None):\n",
    "        if memo is None:\n",
    "            memo = {}\n",
    "        if id(module) in memo:\n",
    "            return memo[id(module)]\n",
    "        \n",
    "        clone = copy.deepcopy(module)\n",
    "        memo[id(module)] = clone\n",
    "        \n",
    "        return clone\n",
    "    \n",
    "    def _register_hooks(self, module, activations, relevances):\n",
    "        forward_hooks = []\n",
    "        backward_hooks = []\n",
    "        \n",
    "        def forward_hook(m, input, output):\n",
    "            activations[id(m)] = output.detach()\n",
    "            \n",
    "        def backward_hook(m, grad_in, grad_out):\n",
    "            if id(m) in activations:\n",
    "                with torch.no_grad():\n",
    "                    a = activations[id(m)]\n",
    "                    if isinstance(m, nn.Conv2d):\n",
    "                        if m.stride == (1, 1) and m.padding == (1, 1):\n",
    "                            w = m.weight\n",
    "                            w_pos = torch.clamp(w, min=0)\n",
    "                            z = torch.nn.functional.conv2d(a, w_pos, bias=None, \n",
    "                                                          stride=m.stride, padding=m.padding)\n",
    "                            s = (grad_out[0] / (z + self.epsilon)).data\n",
    "                            c = torch.nn.functional.conv_transpose2d(s, w_pos, \n",
    "                                                                    stride=m.stride, padding=m.padding)\n",
    "                            relevances[id(m)] = (a * c).data\n",
    "                        else:\n",
    "                            relevances[id(m)] = (a * grad_out[0]).data\n",
    "                    elif isinstance(m, nn.Linear):\n",
    "                        w = m.weight\n",
    "                        w_pos = torch.clamp(w, min=0)\n",
    "                        z = torch.matmul(a, w_pos.t())\n",
    "                        s = (grad_out[0] / (z + self.epsilon)).data\n",
    "                        c = torch.matmul(s, w_pos)\n",
    "                        relevances[id(m)] = (a * c).data\n",
    "                    else:\n",
    "                        relevances[id(m)] = (a * grad_out[0]).data\n",
    "        \n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear, nn.BatchNorm2d, nn.ReLU)):\n",
    "            forward_hooks.append(module.register_forward_hook(forward_hook))\n",
    "            backward_hooks.append(module.register_full_backward_hook(backward_hook))\n",
    "        \n",
    "        for child in module.children():\n",
    "            f_hooks, b_hooks = self._register_hooks(child, activations, relevances)\n",
    "            forward_hooks.extend(f_hooks)\n",
    "            backward_hooks.extend(b_hooks)\n",
    "            \n",
    "        return forward_hooks, backward_hooks\n",
    "    \n",
    "    def __call__(self, input_tensor, target_class=None):\n",
    "        input_tensor = input_tensor.clone().detach().to(device)\n",
    "        input_tensor.requires_grad = True\n",
    "        \n",
    "        activations = {}\n",
    "        relevances = {}\n",
    "        \n",
    "        forward_hooks, backward_hooks = self._register_hooks(self.model, activations, relevances)\n",
    "        \n",
    "        try:\n",
    "            output = self.model(input_tensor)\n",
    "            \n",
    "            if target_class is None:\n",
    "                target_class = torch.argmax(output, dim=1).item()\n",
    "            \n",
    "            one_hot = torch.zeros_like(output)\n",
    "            one_hot[0, target_class] = 1.0\n",
    "            \n",
    "            self.model.zero_grad()\n",
    "            output.backward(gradient=one_hot, retain_graph=True)\n",
    "            \n",
    "            input_gradient = input_tensor.grad.data\n",
    "            \n",
    "            first_layer_id = None\n",
    "            for module in self.model.modules():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    first_layer_id = id(module)\n",
    "                    break\n",
    "            \n",
    "            if first_layer_id in relevances:\n",
    "                relevance_map = relevances[first_layer_id]\n",
    "            else:\n",
    "                relevance_map = input_gradient\n",
    "                \n",
    "            relevance_map = relevance_map.sum(dim=1).squeeze()\n",
    "            \n",
    "            relevance_map = torch.abs(relevance_map)\n",
    "            if torch.max(relevance_map) > 0:\n",
    "                relevance_map = relevance_map / torch.max(relevance_map)\n",
    "            \n",
    "            return relevance_map.cpu().numpy()\n",
    "            \n",
    "        finally:\n",
    "            for hook in forward_hooks + backward_hooks:\n",
    "                hook.remove()\n",
    "\n",
    "def apply_lrp(model, img_tensor, img_np, target_class=None):\n",
    "    lrp = LRP(model)\n",
    "    \n",
    "    relevance_map = lrp(img_tensor, target_class)\n",
    "    \n",
    "    relevance_resized = cv2.resize(relevance_map, (img_np.shape[1], img_np.shape[0]))\n",
    "    \n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * relevance_resized), cv2.COLORMAP_JET)\n",
    "    \n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    alpha = 0.4\n",
    "    visualization = heatmap * alpha + img_np * (1 - alpha)\n",
    "    visualization = np.uint8(visualization)\n",
    "    \n",
    "    return visualization, relevance_map\n",
    "\n",
    "def visualize_lrp(model, dataloader, class_names, num_images=5):\n",
    "    model.eval()\n",
    "    \n",
    "    images, labels = next(iter(dataloader))\n",
    "    images = images[:num_images]\n",
    "    labels = labels[:num_images]\n",
    "    \n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))\n",
    "    \n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        img_np = image.cpu().numpy().transpose(1, 2, 0)\n",
    "        img_np = np.clip(img_np * np.array([0.229, 0.224, 0.225]) + \n",
    "                        np.array([0.485, 0.456, 0.406]), 0, 1)\n",
    "        \n",
    "        input_tensor = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            prob = torch.nn.functional.softmax(output, dim=1)\n",
    "        \n",
    "        true_lrp, _ = apply_lrp(model, input_tensor, img_np, label.item())\n",
    "        \n",
    "        pred_lrp, _ = apply_lrp(model, input_tensor, img_np, pred.item())\n",
    "        \n",
    "        axes[i, 0].imshow(img_np)\n",
    "        axes[i, 0].set_title(f\"True: {class_names[label]}\\nPred: {class_names[pred]} ({prob[0][pred.item()]:.2f})\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(true_lrp)\n",
    "        axes[i, 1].set_title(f\"LRP for {class_names[label]}\")\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(pred_lrp)\n",
    "        axes[i, 2].set_title(f\"LRP for {class_names[pred]}\")\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lrp_visualizations.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920dcb7f",
   "metadata": {},
   "source": [
    "### Methods Comparison: GradCAM vs LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdac0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_xai_methods(model, dataloader, class_names, num_images=3):\n",
    "    model.eval()\n",
    "    \n",
    "    images, labels = next(iter(dataloader))\n",
    "    images = images[:num_images]\n",
    "    labels = labels[:num_images]\n",
    "    \n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))\n",
    "    \n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        img_np = image.cpu().numpy().transpose(1, 2, 0)\n",
    "        img_np = np.clip(img_np * np.array([0.229, 0.224, 0.225]) + \n",
    "                        np.array([0.485, 0.456, 0.406]), 0, 1)\n",
    "        \n",
    "        input_tensor = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            prob = torch.nn.functional.softmax(output, dim=1)\n",
    "        \n",
    "        gradcam_vis, _ = apply_gradcam(model, input_tensor, img_np, pred.item())\n",
    "        \n",
    "        lrp_vis, _ = apply_lrp(model, input_tensor, img_np, pred.item())\n",
    "        \n",
    "        axes[i, 0].imshow(img_np)\n",
    "        axes[i, 0].set_title(f\"Original\\nTrue: {class_names[label]}\\nPred: {class_names[pred]} ({prob[0][pred.item()]:.2f})\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(gradcam_vis)\n",
    "        axes[i, 1].set_title(\"GradCAM\")\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(lrp_vis)\n",
    "        axes[i, 2].set_title(\"Layer-wise Relevance Propagation\")\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xai_comparison.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
