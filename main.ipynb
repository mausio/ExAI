{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ExAI - Explainable Corgy (Cardigan) seperator ğŸ¶\n",
        "\n",
        "We use [Contrastive GradCAM](https://xai-blog.netlify.app/docs/groups/contrastive-grad-cam-consistency/#contrastive-grad-cam-consistency-loss)\n",
        "and [Layerwise Relevance Propagation](https://github.com/kaifishr/PyTorchRelevancePropagation) to explain the difference between Corgys and Cardigans.\n",
        "\n",
        " - We leverage [Standford ImageNet Dog Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) for fintuning [ResNet](https://pytorch.org/hub/pytorch_vision_resnet/#model-description).\n",
        " - suspects: [Pembroke](https://de.wikipedia.org/wiki/Welsh_Corgi_Pembroke) | [Cardigan](https://de.wikipedia.org/wiki/Welsh_Corgi_Cardigan)\n",
        "\n",
        "## The Process..\n",
        "\n",
        " 1. Data/Dependency Loading and Transformation.\n",
        " 2. Model Definition.\n",
        " 3. Loss Functions and Optimization definitions.\n",
        " 4. Actual Training loop.\n",
        " 5. Contrastive Grad-CAM Visualization.\n",
        " 6. Layerwise Relevance Propagation.\n",
        " 7. Save finetuned model.\n",
        "\n"
      ],
      "metadata": {
        "id": "GSjCu3qxLzNK"
      },
      "id": "GSjCu3qxLzNK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data/Dependency Loading and Transformation"
      ],
      "metadata": {
        "id": "dJH_W2oaOJoB"
      },
      "id": "dJH_W2oaOJoB"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies.. [once]\n",
        "%pip install typing matplotlib numpy pandas torch torchvision tqdm -q"
      ],
      "metadata": {
        "id": "uAZjQIg1RNEt"
      },
      "id": "uAZjQIg1RNEt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load up dependencies.. [every run]\n",
        "from typing import Dict\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Numpy\n",
        "import numpy as np\n",
        "\n",
        "# Pandas\n",
        "import pandas as pd\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "# Progress bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "# Datasets\n",
        "from torchvision.datasets import Imagenette\n",
        "\n",
        "# Datasets utils\n",
        "from torchvision.datasets.utils import download_url, extract_archive\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Active device for training:\", device)"
      ],
      "metadata": {
        "id": "a6bPBrZtQvR0",
        "outputId": "6501edec-c30f-4a80-b3ce-128ff28b9c00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "a6bPBrZtQvR0",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/78.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”\u001b[0m \u001b[32m71.7/78.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m837.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Active device for training: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SJYyGFcNMcRI",
        "outputId": "2d1015e3-417b-4d9d-a47e-54b95ffc60f1",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "/\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "-= Done =-\n"
          ]
        }
      ],
      "source": [
        "# If you want to persist data, you have to store it on g-drive or github.\n",
        "# ..unfortunately you have to connect to g-drive manually (once every session).\n",
        "#@title Setup Google Drive for persistent storage [optional | every run]\n",
        "\n",
        "OPTIONS = {}\n",
        "\n",
        "USE_GOOGLE_DRIVE = True  #@param {type:\"boolean\"}\n",
        "\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    !echo \"Mounting Google Drive...\"\n",
        "    %cd /\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "!echo -= Done =-\n"
      ],
      "id": "SJYyGFcNMcRI"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id",
        "outputId": "6c3b9c7f-2926-4caf-e2a3-84370e60329b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded images.tar to /content/drive/MyDrive/xAI-Corgis/images.tar\n"
          ]
        }
      ],
      "source": [
        "#@title Download training data [once]\n",
        "\n",
        "DOWNLOAD_DIR = \"/content/drive/MyDrive/xAI-Corgis\" # @param{type:\"string\"}\n",
        "\n",
        "import os\n",
        "import requests\n",
        "\n",
        "def download_data(url, path):\n",
        "    \"\"\"\n",
        "    Downloads data from a given URL and stores it in a designated path.\n",
        "\n",
        "    Args:\n",
        "        url: The URL of the data to download.\n",
        "        path: The path to store the downloaded data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "\n",
        "    # Download the data\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "    # Save the data to the specified path\n",
        "    filename = os.path.basename(url)\n",
        "    filepath = os.path.join(path, filename)\n",
        "\n",
        "    with open(filepath, 'wb') as file:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            file.write(chunk)\n",
        "\n",
        "    print(f\"Downloaded {filename} to {filepath}\")\n",
        "\n",
        "\n",
        "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
        "\n",
        "download_data(\"http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\", DOWNLOAD_DIR)\n",
        "# download_data(\"http://vision.stanford.edu/aditya86/ImageNetDogs/train_data.mat\", TRAINING_SETS_TARGET)\n",
        "# download_data(\"http://vision.stanford.edu/aditya86/ImageNetDogs/test_data.mat\", TRAINING_SETS_TARGET)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title"
      ],
      "metadata": {
        "id": "OTRIYp9TRgrT"
      },
      "id": "OTRIYp9TRgrT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract training data to target [every training run]\n",
        "\n",
        "DOWNLOAD_DIR = \"/content/drive/MyDrive/xAI-Corgis\" # @param{type:\"string\"}\n",
        "EXTRACT_TRAINING_SETS_TO = \"/content/dogs\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "os.makedirs(EXTRACT_TRAINING_SETS_TO, exist_ok=True)\n",
        "\n",
        "for file in os.listdir(DOWNLOAD_DIR):\n",
        "    if file.endswith(\".tar\"):\n",
        "        file_path = os.path.join(DOWNLOAD_DIR, file)\n",
        "        with tarfile.open(file_path, \"r\") as tar:\n",
        "            tar.extractall(path=EXTRACT_TRAINING_SETS_TO)\n",
        "            print(f\"{file} successfully extracted to: '{EXTRACT_TRAINING_SETS_TO}'.\")"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48J5ZvoSk-Om",
        "outputId": "558113be-691d-477d-b756-670b7e5bc477"
      },
      "id": "48J5ZvoSk-Om",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images.tar successfully extracted to: '/content/dogs'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model Definition"
      ],
      "metadata": {
        "id": "s37ot4N9OQGQ"
      },
      "id": "s37ot4N9OQGQ"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.1. Prepare fine tuning of ResNet50 in PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import scipy.io as sio\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming you have the .mat files downloaded as shown in the previous code.\n",
        "\n",
        "# Define paths to your .mat files\n",
        "train_data_path = os.path.join(TRAINING_SETS_TARGET, 'train_data.mat')\n",
        "test_data_path = os.path.join(TRAINING_SETS_TARGET, 'test_data.mat')\n",
        "\n",
        "# Custom Dataset class to handle .mat files\n",
        "class MatDataset(Dataset):\n",
        "    def __init__(self, mat_file_path, transform=None, image_field_name=\"images\", label_fiels_name=\"labels\"):\n",
        "      \"\"\"\n",
        "        Please make sure your field_names in your .mat file are according to source, wont be checked!\n",
        "      \"\"\"\n",
        "      self.data = sio.loadmat(mat_file_path) # Load .mat file using scipy.io\n",
        "      self.images = self.data[image_field_name] # Adapt based on actual .mat structure\n",
        "      self.labels = self.data[label_fiels_name] # \"\n",
        "      self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      image = self.images[idx]\n",
        "      label = self.labels[idx]\n",
        "\n",
        "      if self.transform:\n",
        "          image = self.transform(image)\n",
        "      return image, label\n",
        "\n",
        "# Data preprocessing and augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = MatDataset(train_data_path, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataset = MatDataset(test_data_path, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load pre-trained ResNet50 model\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Modify the final fully connected layer for your specific number of classes\n",
        "num_classes = len(set(train_dataset.labels)) # Replace with your number of classes\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "# Define loss function, optimizer, and device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using following device: {device}\")\n",
        "\n",
        "# load..\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "X71aR-0Y5O-7",
        "cellView": "form"
      },
      "id": "X71aR-0Y5O-7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Run Fine-tuning\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Fine-tuning loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluation (example)\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test data: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "o-2VF4oKDqRw"
      },
      "id": "o-2VF4oKDqRw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save the finetuned model\n",
        "torch.save(model.state_dict(), 'resnet50_finetuned.pth')"
      ],
      "metadata": {
        "id": "TssTfUXeH_C_"
      },
      "id": "TssTfUXeH_C_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Loss Functions and Optimization definitions.\n"
      ],
      "metadata": {
        "id": "T7m2XxlQO6RM"
      },
      "id": "T7m2XxlQO6RM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Actual Training loop"
      ],
      "metadata": {
        "id": "JKGE_JEZO-R3"
      },
      "id": "JKGE_JEZO-R3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Contrastive Grad-CAM Visualization\n",
        "\n",
        "Explanation by transforming same images and evaluating the result.  \n",
        "TODO: Preferably by applying a before/after view."
      ],
      "metadata": {
        "id": "-7NdzUHoO-jY"
      },
      "id": "-7NdzUHoO-jY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Layerwise Relevance Propagation"
      ],
      "metadata": {
        "id": "ROv2-RjZO-2s"
      },
      "id": "ROv2-RjZO-2s"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Save finteuned model"
      ],
      "metadata": {
        "id": "kuWxKCRTO_Ef"
      },
      "id": "kuWxKCRTO_Ef"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}