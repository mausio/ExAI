{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "GSjCu3qxLzNK",
      "metadata": {
        "id": "GSjCu3qxLzNK"
      },
      "source": [
        "# ExAI - Explainable Corgi (Cardigan) seperator 🐶\n",
        "\n",
        "We use [Contrastive GradCAM](https://xai-blog.netlify.app/docs/groups/contrastive-grad-cam-consistency/#contrastive-grad-cam-consistency-loss)\n",
        "and [Layerwise Relevance Propagation](https://github.com/kaifishr/PyTorchRelevancePropagation) to explain the difference between Corgys and Cardigans.\n",
        "\n",
        " - We leverage [Standford ImageNet Dog Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) for fintuning [ResNet](https://pytorch.org/hub/pytorch_vision_resnet/#model-description).\n",
        " - suspects: [Pembroke](https://de.wikipedia.org/wiki/Welsh_Corgi_Pembroke) | [Cardigan](https://de.wikipedia.org/wiki/Welsh_Corgi_Cardigan)\n",
        "\n",
        "## The Process: \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dJH_W2oaOJoB",
      "metadata": {
        "id": "dJH_W2oaOJoB"
      },
      "source": [
        "## 1. Data/Dependency Loading and Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11000ff2",
      "metadata": {},
      "source": [
        "### Import of all necessary packages/libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a6bPBrZtQvR0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6bPBrZtQvR0",
        "outputId": "6501edec-c30f-4a80-b3ce-128ff28b9c00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m71.7/78.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m837.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Active device for training: cpu\n"
          ]
        }
      ],
      "source": [
        "# Setup and Imports\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets.utils import download_url, extract_archive\n",
        "\n",
        "# For evaluation metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "932de5c0",
      "metadata": {},
      "source": [
        "### Setting device to accelerate processing process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "SJYyGFcNMcRI",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SJYyGFcNMcRI",
        "outputId": "2d1015e3-417b-4d9d-a47e-54b95ffc60f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounting Google Drive...\n",
            "/\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "-= Done =-\n"
          ]
        }
      ],
      "source": [
        "# Setup device for training\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "018e2423",
      "metadata": {},
      "source": [
        "### Downloading Dataset if not already in directory\n",
        "[Stanford Dogs Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/):\n",
        "\n",
        "> The Stanford Dogs dataset contains images of 120 breeds of dogs from around the world. This dataset has been built using images and annotation from ImageNet for the task of fine-grained image categorization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "initial_id",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "initial_id",
        "outputId": "6c3b9c7f-2926-4caf-e2a3-84370e60329b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded images.tar to /content/drive/MyDrive/xAI-Corgis/images.tar\n"
          ]
        }
      ],
      "source": [
        "# Dataset Downloading and Extraction\n",
        "def download_and_extract_dataset(download_dir, extract_dir):\n",
        "    \"\"\"\n",
        "    Downloads and extracts the Stanford Dogs Dataset\n",
        "    \"\"\"\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "    \n",
        "    # Download the dataset\n",
        "    dataset_url = \"http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\"\n",
        "    filename = os.path.basename(dataset_url)\n",
        "    filepath = os.path.join(download_dir, filename)\n",
        "    \n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        download_url(dataset_url, download_dir)\n",
        "    else:\n",
        "        print(f\"File {filename} already exists in {download_dir}\")\n",
        "    \n",
        "    # Extract the dataset\n",
        "    if not os.path.exists(os.path.join(extract_dir, \"Images\")):\n",
        "        print(f\"Extracting {filename} to {extract_dir}...\")\n",
        "        extract_archive(filepath, extract_dir)\n",
        "    else:\n",
        "        print(f\"Dataset already extracted to {extract_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b84b374a",
      "metadata": {},
      "source": [
        "### Corgi Dataset Class "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca1af7c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class CorgiDataset(Dataset):\n",
        "    def __init__(self, dataset_root, transform=None):\n",
        "        \"\"\"\n",
        "        Custom dataset for Pembroke and Cardigan Welsh Corgis\n",
        "        \n",
        "        Args:\n",
        "            dataset_root: Root directory containing the Images folder\n",
        "            transform: PyTorch transforms for data augmentation\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.class_names = ['Pembroke', 'Cardigan']\n",
        "        \n",
        "        # Find the corgi breed directories\n",
        "        images_dir = os.path.join(dataset_root, \"Images\")\n",
        "        if not os.path.exists(images_dir):\n",
        "            raise FileNotFoundError(f\"Images directory not found at {images_dir}\")\n",
        "            \n",
        "        all_breeds = os.listdir(images_dir)\n",
        "        \n",
        "        pembroke_dir = None\n",
        "        cardigan_dir = None\n",
        "        \n",
        "        for breed in all_breeds:\n",
        "            if \"Pembroke\" in breed:\n",
        "                pembroke_dir = os.path.join(images_dir, breed)\n",
        "            elif \"Cardigan\" in breed:\n",
        "                cardigan_dir = os.path.join(images_dir, breed)\n",
        "        \n",
        "        if not pembroke_dir or not cardigan_dir:\n",
        "            raise ValueError(\"Could not find Pembroke or Cardigan directories\")\n",
        "        \n",
        "        print(f\"Pembroke directory: {pembroke_dir}\")\n",
        "        print(f\"Cardigan directory: {cardigan_dir}\")\n",
        "        \n",
        "        # Load Pembroke images (label 0)\n",
        "        for img_name in os.listdir(pembroke_dir):\n",
        "            if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                self.image_paths.append(os.path.join(pembroke_dir, img_name))\n",
        "                self.labels.append(0)  # Pembroke\n",
        "        \n",
        "        # Load Cardigan images (label 1)\n",
        "        for img_name in os.listdir(cardigan_dir):\n",
        "            if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                self.image_paths.append(os.path.join(cardigan_dir, img_name))\n",
        "                self.labels.append(1)  # Cardigan\n",
        "        \n",
        "        # Print dataset statistics\n",
        "        print(f\"Total number of images: {len(self.image_paths)}\")\n",
        "        print(f\"Pembroke images: {sum(1 for label in self.labels if label == 0)}\")\n",
        "        print(f\"Cardigan images: {sum(1 for label in self.labels if label == 1)}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            label = self.labels[idx]\n",
        "            \n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "                \n",
        "            return image, label\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            # Return a blank image and the same label\n",
        "            blank_image = torch.zeros((3, 224, 224)) if self.transform else Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "            return blank_image, self.labels[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "738a38e9",
      "metadata": {},
      "source": [
        "### Class for transforming Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "48J5ZvoSk-Om",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48J5ZvoSk-Om",
        "outputId": "558113be-691d-477d-b756-670b7e5bc477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images.tar successfully extracted to: '/content/dogs'.\n"
          ]
        }
      ],
      "source": [
        "class TransformedSubset(Dataset):\n",
        "    \"\"\"Wrapper for applying transforms to a subset of the dataset\"\"\"\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Correctly handle the subset indexing\n",
        "        image, label = self.subset.dataset[self.subset.indices[idx]]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.subset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c783f4d6",
      "metadata": {},
      "source": [
        "### Data Preparation and Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fba4851",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def prepare_dataloaders(dataset_root, batch_size=32, num_workers=2):\n",
        "    \"\"\"\n",
        "    Prepares the dataloaders for training and validation\n",
        "    \n",
        "    Args:\n",
        "        dataset_root: Root directory containing the Images folder\n",
        "        batch_size: Batch size for training\n",
        "        num_workers: Number of workers for data loading\n",
        "        \n",
        "    Returns:\n",
        "        train_loader: DataLoader for training\n",
        "        val_loader: DataLoader for validation\n",
        "        class_names: Names of the classes\n",
        "    \"\"\"\n",
        "    # Data transformations\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "    \n",
        "    # Create the dataset\n",
        "    dataset = CorgiDataset(dataset_root, transform=None)  # No transform yet\n",
        "    \n",
        "    # Split into training and validation sets (80% train, 20% validation)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    \n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    \n",
        "    # Create new datasets with appropriate transforms\n",
        "    train_dataset_transformed = TransformedSubset(train_dataset, data_transforms['train'])\n",
        "    val_dataset_transformed = TransformedSubset(val_dataset, data_transforms['val'])\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset_transformed, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=True, \n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset_transformed, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=False, \n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "    \n",
        "    print(f\"Training set size: {len(train_dataset)} images\")\n",
        "    print(f\"Validation set size: {len(val_dataset)} images\")\n",
        "    print(f\"Training batches: {len(train_loader)}\")\n",
        "    print(f\"Validation batches: {len(val_loader)}\")\n",
        "    \n",
        "    return train_loader, val_loader, dataset.class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s37ot4N9OQGQ",
      "metadata": {
        "id": "s37ot4N9OQGQ"
      },
      "source": [
        "## 2. Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d7441e4",
      "metadata": {},
      "source": [
        "### Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X71aR-0Y5O-7",
      "metadata": {
        "cellView": "form",
        "id": "X71aR-0Y5O-7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def setup_model(num_classes=2):\n",
        "    \"\"\"\n",
        "    Sets up the ResNet50 model for fine-tuning\n",
        "    \n",
        "    Args:\n",
        "        num_classes: Number of output classes\n",
        "        \n",
        "    Returns:\n",
        "        model: The ResNet50 model configured for fine-tuning\n",
        "    \"\"\"\n",
        "    # Load pre-trained ResNet50\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    \n",
        "    # Freeze early layers\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    # Unfreeze later layers for fine-tuning\n",
        "    for param in model.layer4.parameters():\n",
        "        param.requires_grad = True\n",
        "    \n",
        "    # Replace the final fully connected layer\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Linear(num_features, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(512, num_classes)\n",
        "    )\n",
        "    \n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Print model information\n",
        "    print(f\"ResNet50 model configured for {num_classes} classes\")\n",
        "    print(f\"Trainable parameters in model.layer4: {sum(p.numel() for p in model.layer4.parameters() if p.requires_grad)}\")\n",
        "    print(f\"Trainable parameters in model.fc: {sum(p.numel() for p in model.fc.parameters() if p.requires_grad)}\")\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67cef616",
      "metadata": {},
      "source": [
        "### Training Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b0b4b0b",
      "metadata": {},
      "source": [
        "#### Traning Function: Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o-2VF4oKDqRw",
      "metadata": {
        "id": "o-2VF4oKDqRw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer):\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch\n",
        "    \n",
        "    Args:\n",
        "        model: The model to train\n",
        "        dataloader: DataLoader for training data\n",
        "        criterion: Loss function\n",
        "        optimizer: Optimizer\n",
        "        \n",
        "    Returns:\n",
        "        epoch_loss: Average loss for the epoch\n",
        "        epoch_acc: Accuracy for the epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    \n",
        "    # Iterate over data\n",
        "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward + optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "    \n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "    \n",
        "    print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "    \n",
        "    return epoch_loss, epoch_acc.item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26f1783f",
      "metadata": {},
      "source": [
        "#### Training Function: Validate Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TssTfUXeH_C_",
      "metadata": {
        "id": "TssTfUXeH_C_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def validate_epoch(model, dataloader, criterion):\n",
        "    \"\"\"\n",
        "    Validates the model for one epoch\n",
        "    \n",
        "    Args:\n",
        "        model: The model to validate\n",
        "        dataloader: DataLoader for validation data\n",
        "        criterion: Loss function\n",
        "        \n",
        "    Returns:\n",
        "        epoch_loss: Average loss for the epoch\n",
        "        epoch_acc: Accuracy for the epoch\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluate mode\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    \n",
        "    # Iterate over data\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Validation\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Forward\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "    \n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "    \n",
        "    print(f'Validation Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "    \n",
        "    return epoch_loss, epoch_acc.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f19d3918",
      "metadata": {},
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "419a7c8d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=15, patience=5):\n",
        "    \"\"\"\n",
        "    Trains the model and tracks performance metrics\n",
        "    \n",
        "    Args:\n",
        "        model: The model to train\n",
        "        train_loader: DataLoader for training data\n",
        "        val_loader: DataLoader for validation data\n",
        "        num_epochs: Number of epochs to train for\n",
        "        patience: Number of epochs to wait for improvement before early stopping\n",
        "        \n",
        "    Returns:\n",
        "        model: The trained model\n",
        "        history: Dictionary containing loss and accuracy history\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': model.layer4.parameters(), 'lr': 1e-4},\n",
        "        {'params': model.fc.parameters(), 'lr': 1e-3}\n",
        "    ], weight_decay=1e-5)\n",
        "    \n",
        "    # Learning rate scheduler\n",
        "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "    \n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    no_improve_epochs = 0\n",
        "    \n",
        "    # History for tracking metrics\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 40)\n",
        "        \n",
        "        # Training phase\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        \n",
        "        # Validation phase\n",
        "        val_loss, val_acc = validate_epoch(model, val_loader, criterion)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        \n",
        "        # Adjust learning rate based on validation loss\n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        # Deep copy the model if best accuracy\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "        \n",
        "        print(f'Best val Acc: {best_acc:.4f}')\n",
        "        \n",
        "        # Early stopping\n",
        "        if no_improve_epochs >= patience:\n",
        "            print(f'Early stopping after {epoch+1} epochs without improvement')\n",
        "            break\n",
        "        \n",
        "        print()\n",
        "    \n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "    \n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "676e3395",
      "metadata": {},
      "source": [
        "### Evaluating Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d52031ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_model(model, dataloader, class_names):\n",
        "    \"\"\"\n",
        "    Evaluates the model on a dataset and computes metrics\n",
        "    \n",
        "    Args:\n",
        "        model: The model to evaluate\n",
        "        dataloader: DataLoader for the evaluation data\n",
        "        class_names: Names of the classes\n",
        "        \n",
        "    Returns:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        report: Classification report\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            \n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "    \n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print classification report\n",
        "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report)\n",
        "    \n",
        "    return y_true, y_pred, report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21d22702",
      "metadata": {},
      "source": [
        "### Plotting the History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d76652b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plots the training history\n",
        "    \n",
        "    Args:\n",
        "        history: Dictionary containing loss and accuracy history\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
        "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "759c1883",
      "metadata": {},
      "source": [
        "### Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5d71d52",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def save_model(model, save_path, class_names, optimizer=None, epoch=None, history=None):\n",
        "    \"\"\"\n",
        "    Saves the model to a file in multiple formats\n",
        "    \n",
        "    Args:\n",
        "        model: The model to save\n",
        "        save_path: Path to save the model\n",
        "        class_names: Names of the classes\n",
        "        optimizer: Optimizer to save (optional)\n",
        "        epoch: Current epoch (optional)\n",
        "        history: Training history (optional)\n",
        "    \"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    \n",
        "    # Save complete model for inference\n",
        "    torch.save(model, save_path.replace('.pth', '_full.pth'))\n",
        "    print(f\"Complete model saved to: {save_path.replace('.pth', '_full.pth')}\")\n",
        "    \n",
        "    # Save model weights only\n",
        "    torch.save(model.state_dict(), save_path.replace('.pth', '_weights.pth'))\n",
        "    print(f\"Model weights saved to: {save_path.replace('.pth', '_weights.pth')}\")\n",
        "    \n",
        "    # Save checkpoint with additional information\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'classes': class_names,\n",
        "    }\n",
        "    \n",
        "    if optimizer:\n",
        "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "    \n",
        "    if epoch is not None:\n",
        "        checkpoint['epoch'] = epoch\n",
        "        \n",
        "    if history:\n",
        "        checkpoint['history'] = history\n",
        "    \n",
        "    torch.save(checkpoint, save_path)\n",
        "    print(f\"Checkpoint saved to: {save_path}\")\n",
        "    \n",
        "    # Export to ONNX format for deployment\n",
        "    try:\n",
        "        dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "        torch.onnx.export(\n",
        "            model,\n",
        "            dummy_input,\n",
        "            save_path.replace('.pth', '.onnx'),\n",
        "            export_params=True,\n",
        "            opset_version=11,\n",
        "            do_constant_folding=True,\n",
        "            input_names=['input'],\n",
        "            output_names=['output'],\n",
        "            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        "        )\n",
        "        print(f\"ONNX model saved to: {save_path.replace('.pth', '.onnx')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error exporting to ONNX format: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26cb0f01",
      "metadata": {},
      "source": [
        "### Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c4808e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def load_model(load_path, model=None):\n",
        "    \"\"\"\n",
        "    Loads a model from a file\n",
        "    \n",
        "    Args:\n",
        "        load_path: Path to load the model from\n",
        "        model: Model to load weights into (optional)\n",
        "        \n",
        "    Returns:\n",
        "        model: The loaded model\n",
        "        checkpoint: The loaded checkpoint\n",
        "    \"\"\"\n",
        "    try:\n",
        "        checkpoint = torch.load(load_path, map_location=device)\n",
        "        \n",
        "        if model is None:\n",
        "            # Try loading the full model\n",
        "            if load_path.endswith('_full.pth'):\n",
        "                model = torch.load(load_path, map_location=device)\n",
        "                print(f\"Full model loaded from: {load_path}\")\n",
        "                return model, None\n",
        "            \n",
        "            # Otherwise create a new model\n",
        "            model = setup_model()\n",
        "        \n",
        "        # Load state dict if it exists\n",
        "        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        else:\n",
        "            model.load_state_dict(checkpoint)\n",
        "        \n",
        "        print(f\"Model weights loaded from: {load_path}\")\n",
        "        return model, checkpoint\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None, None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc8f6bdd",
      "metadata": {},
      "source": [
        "## 3. Model Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8acc97f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    # Directory setup\n",
        "    download_dir = \"/content/drive/MyDrive/xAI-Corgis\" # TODO: Change to local directory\n",
        "    extract_dir = \"/content/dogs\" # TODO: Change to local directory \n",
        "    \n",
        "    # Download and extract dataset\n",
        "    download_and_extract_dataset(download_dir, extract_dir)\n",
        "    \n",
        "    # Prepare dataloaders\n",
        "    train_loader, val_loader, class_names = prepare_dataloaders(extract_dir, batch_size=32)\n",
        "    \n",
        "    # Setup model\n",
        "    model = setup_model(num_classes=len(class_names))\n",
        "    \n",
        "    # Check if model exists and load it\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "    model_path = os.path.join(download_dir, 'resnet50_corgi_classifier.pth')\n",
        "    if os.path.exists(model_path):\n",
        "        print(\"Loading pre-trained model...\")\n",
        "        model, checkpoint = load_model(model_path)\n",
        "    else:\n",
        "        # Train model\n",
        "        print(\"Training a new model...\")\n",
        "        model, history = train_model(model, train_loader, val_loader, num_epochs=15)\n",
        "        \n",
        "        # Plot training history\n",
        "        plot_training_history(history)\n",
        "        \n",
        "        # Evaluate model\n",
        "        y_true, y_pred, report = evaluate_model(model, val_loader, class_names)\n",
        "        \n",
        "        # Save model\n",
        "        save_model(\n",
        "            model, \n",
        "            model_path,\n",
        "            class_names=class_names,\n",
        "            history=history\n",
        "        )\n",
        "\n",
        "    # Apply XAI methods\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Applying XAI Methods for Model Interpretability\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Visualize with GradCAM\n",
        "    print(\"\\nGenerating GradCAM visualizations...\")\n",
        "    visualize_gradcam(model, val_loader, class_names, num_images=5)\n",
        "    \n",
        "    # Visualize with LRP\n",
        "    print(\"\\nGenerating Layer-wise Relevance Propagation visualizations...\")\n",
        "    visualize_lrp(model, val_loader, class_names, num_images=5)\n",
        "    \n",
        "    # Compare both XAI methods\n",
        "    print(\"\\nComparing GradCAM and LRP methods...\")\n",
        "    compare_xai_methods(model, val_loader, class_names, num_images=3)\n",
        "    \n",
        "    print(\"\\nXAI visualization complete. All results saved as PNG files.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "310ad237",
      "metadata": {},
      "source": [
        "## 4. xAI Methods "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8def829",
      "metadata": {},
      "source": [
        "### GradCAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "530639fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        \"\"\"\n",
        "        Initializes GradCAM with a model and target layer\n",
        "        \n",
        "        Args:\n",
        "            model: The trained PyTorch model\n",
        "            target_layer: The convolutional layer to use for generating the CAM\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.hooks = []\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "        self.register_hooks()\n",
        "        self.model.eval()\n",
        "        \n",
        "    def register_hooks(self):\n",
        "        \"\"\"Registers forward and backward hooks to the target layer\"\"\"\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations = output.detach()\n",
        "            \n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients = grad_output[0].detach()\n",
        "            \n",
        "        # Register the hooks\n",
        "        forward_handle = self.target_layer.register_forward_hook(forward_hook)\n",
        "        backward_handle = self.target_layer.register_full_backward_hook(backward_hook)\n",
        "        \n",
        "        # Store the handles for removal later\n",
        "        self.hooks = [forward_handle, backward_handle]\n",
        "        \n",
        "    def remove_hooks(self):\n",
        "        \"\"\"Removes all registered hooks\"\"\"\n",
        "        for hook in self.hooks:\n",
        "            hook.remove()\n",
        "            \n",
        "    def __call__(self, input_tensor, target_class=None):\n",
        "        \"\"\"\n",
        "        Generates the Grad-CAM for the input tensor\n",
        "        \n",
        "        Args:\n",
        "            input_tensor: Input image (must be normalized the same way as training data)\n",
        "            target_class: Target class index. If None, uses the predicted class.\n",
        "            \n",
        "        Returns:\n",
        "            cam: The normalized Grad-CAM heatmap\n",
        "        \"\"\"\n",
        "        # Forward pass\n",
        "        input_tensor = input_tensor.to(device)\n",
        "        \n",
        "        # Zero gradients\n",
        "        self.model.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        output = self.model(input_tensor)\n",
        "        \n",
        "        # If target_class is None, use predicted class\n",
        "        if target_class is None:\n",
        "            target_class = torch.argmax(output, dim=1).item()\n",
        "        \n",
        "        # One-hot encoding of the target class\n",
        "        one_hot = torch.zeros_like(output)\n",
        "        one_hot[0, target_class] = 1\n",
        "        \n",
        "        # Backward pass to get gradients\n",
        "        output.backward(gradient=one_hot, retain_graph=True)\n",
        "        \n",
        "        # Get mean gradients and activations\n",
        "        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])\n",
        "        \n",
        "        # Weight the activations by the gradients\n",
        "        for i in range(pooled_gradients.shape[0]):\n",
        "            self.activations[:, i, :, :] *= pooled_gradients[i]\n",
        "        \n",
        "        # Average activations over the channel dimension\n",
        "        cam = torch.mean(self.activations, dim=1).squeeze()\n",
        "        \n",
        "        # ReLU on the heatmap\n",
        "        cam = torch.maximum(cam, torch.tensor(0.0).to(device))\n",
        "        \n",
        "        # Normalize the heatmap\n",
        "        if torch.max(cam) > 0:\n",
        "            cam = cam / torch.max(cam)\n",
        "        \n",
        "        # Resize to the input image size\n",
        "        cam = cam.cpu().numpy()\n",
        "        \n",
        "        return cam\n",
        "\n",
        "def apply_gradcam(model, img_tensor, img_np, target_class=None, layer_name='layer4'):\n",
        "    \"\"\"\n",
        "    Applies GradCAM to visualize model attention\n",
        "    \n",
        "    Args:\n",
        "        model: Trained PyTorch model\n",
        "        img_tensor: Input image tensor (1, C, H, W)\n",
        "        img_np: Original numpy image for visualization (RGB)\n",
        "        target_class: Target class for visualization\n",
        "        layer_name: Name of layer to use for GradCAM (default: 'layer4')\n",
        "        \n",
        "    Returns:\n",
        "        visualization: Heatmap overlaid on original image\n",
        "        cam: Raw heatmap\n",
        "    \"\"\"\n",
        "    # Get the target layer\n",
        "    target_layer = model.layer4\n",
        "    \n",
        "    # Create GradCAM instance\n",
        "    grad_cam = GradCAM(model, target_layer)\n",
        "    \n",
        "    # Generate heatmap\n",
        "    cam = grad_cam(img_tensor, target_class)\n",
        "    \n",
        "    # Resize CAM to input image size\n",
        "    cam_resized = cv2.resize(cam, (img_np.shape[1], img_np.shape[0]))\n",
        "    \n",
        "    # Convert to heatmap\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
        "    \n",
        "    # Convert to RGB (from BGR)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Overlay heatmap on original image\n",
        "    alpha = 0.4\n",
        "    visualization = heatmap * alpha + img_np * (1 - alpha)\n",
        "    visualization = np.uint8(visualization)\n",
        "    \n",
        "    # Remove hooks\n",
        "    grad_cam.remove_hooks()\n",
        "    \n",
        "    return visualization, cam\n",
        "\n",
        "def visualize_gradcam(model, dataloader, class_names, num_images=5):\n",
        "    \"\"\"\n",
        "    Visualizes GradCAM for a batch of images\n",
        "    \n",
        "    Args:\n",
        "        model: Trained PyTorch model\n",
        "        dataloader: DataLoader containing images to visualize\n",
        "        class_names: Names of the classes\n",
        "        num_images: Number of images to visualize\n",
        "    \"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Get a batch of images\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images = images[:num_images]\n",
        "    labels = labels[:num_images]\n",
        "    \n",
        "    # Create a figure\n",
        "    fig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))\n",
        "    \n",
        "    for i, (image, label) in enumerate(zip(images, labels)):\n",
        "        # Convert to numpy image for display\n",
        "        img_np = image.cpu().numpy().transpose(1, 2, 0)\n",
        "        img_np = np.clip(img_np * np.array([0.229, 0.224, 0.225]) + \n",
        "                        np.array([0.485, 0.456, 0.406]), 0, 1)\n",
        "        \n",
        "        # Prepare input for model\n",
        "        input_tensor = image.unsqueeze(0).to(device)\n",
        "        \n",
        "        # Get model prediction\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "            _, pred = torch.max(output, 1)\n",
        "            prob = torch.nn.functional.softmax(output, dim=1)\n",
        "        \n",
        "        # Generate GradCAM for true class\n",
        "        true_cam, _ = apply_gradcam(model, input_tensor, img_np, label.item())\n",
        "        \n",
        "        # Generate GradCAM for predicted class\n",
        "        pred_cam, _ = apply_gradcam(model, input_tensor, img_np, pred.item())\n",
        "        \n",
        "        # Display original image\n",
        "        axes[i, 0].imshow(img_np)\n",
        "        axes[i, 0].set_title(f\"True: {class_names[label]}\\nPred: {class_names[pred]} ({prob[0][pred.item()]:.2f})\")\n",
        "        axes[i, 0].axis('off')\n",
        "        \n",
        "        # Display GradCAM for true class\n",
        "        axes[i, 1].imshow(true_cam)\n",
        "        axes[i, 1].set_title(f\"GradCAM for {class_names[label]}\")\n",
        "        axes[i, 1].axis('off')\n",
        "        \n",
        "        # Display GradCAM for predicted class\n",
        "        axes[i, 2].imshow(pred_cam)\n",
        "        axes[i, 2].set_title(f\"GradCAM for {class_names[pred]}\")\n",
        "        axes[i, 2].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('gradcam_visualizations.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa647935",
      "metadata": {},
      "source": [
        "### Layer-wise Relevance Propagation (LRP) Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f3e702f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class LRP:\n",
        "    \"\"\"\n",
        "    Layer-wise Relevance Propagation (LRP) for CNN visualization.\n",
        "    \n",
        "    This class implements the LRP technique to visualize which parts of an image\n",
        "    contribute most to a model's prediction.\n",
        "    \n",
        "    Reference: Bach et al., \"On Pixel-Wise Explanations for Non-Linear Classifier \n",
        "    Decisions by Layer-Wise Relevance Propagation\", https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140\n",
        "    \"\"\"\n",
        "    def __init__(self, model, epsilon=1e-9):\n",
        "        \"\"\"\n",
        "        Initializes LRP with a model\n",
        "        \n",
        "        Args:\n",
        "            model: The trained PyTorch model (ResNet50)\n",
        "            epsilon: Small constant for numerical stability\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.model.eval()\n",
        "        \n",
        "    def _clone_module(self, module, memo=None):\n",
        "        \"\"\"Create a copy of a module by recursively cloning its parameters and buffers\"\"\"\n",
        "        if memo is None:\n",
        "            memo = {}\n",
        "        if id(module) in memo:\n",
        "            return memo[id(module)]\n",
        "        \n",
        "        clone = copy.deepcopy(module)\n",
        "        memo[id(module)] = clone\n",
        "        \n",
        "        return clone\n",
        "    \n",
        "    def _register_hooks(self, module, activations, relevances):\n",
        "        \"\"\"Register forward and backward hooks for LRP\"\"\"\n",
        "        forward_hooks = []\n",
        "        backward_hooks = []\n",
        "        \n",
        "        def forward_hook(m, input, output):\n",
        "            activations[id(m)] = output.detach()\n",
        "            \n",
        "        def backward_hook(m, grad_in, grad_out):\n",
        "            \"\"\"Modified backward pass for LRP\"\"\"\n",
        "            if id(m) in activations:\n",
        "                with torch.no_grad():\n",
        "                    # Get the activations from the forward pass\n",
        "                    a = activations[id(m)]\n",
        "                    if isinstance(m, nn.Conv2d):\n",
        "                        # For convolutional layers\n",
        "                        if m.stride == (1, 1) and m.padding == (1, 1):\n",
        "                            w = m.weight\n",
        "                            w_pos = torch.clamp(w, min=0)\n",
        "                            z = torch.nn.functional.conv2d(a, w_pos, bias=None, \n",
        "                                                          stride=m.stride, padding=m.padding)\n",
        "                            s = (grad_out[0] / (z + self.epsilon)).data\n",
        "                            c = torch.nn.functional.conv_transpose2d(s, w_pos, \n",
        "                                                                    stride=m.stride, padding=m.padding)\n",
        "                            relevances[id(m)] = (a * c).data\n",
        "                        else:\n",
        "                            # For stride > 1 or different padding, use a simpler rule\n",
        "                            relevances[id(m)] = (a * grad_out[0]).data\n",
        "                    elif isinstance(m, nn.Linear):\n",
        "                        # For fully connected layers\n",
        "                        w = m.weight\n",
        "                        w_pos = torch.clamp(w, min=0)\n",
        "                        z = torch.matmul(a, w_pos.t())\n",
        "                        s = (grad_out[0] / (z + self.epsilon)).data\n",
        "                        c = torch.matmul(s, w_pos)\n",
        "                        relevances[id(m)] = (a * c).data\n",
        "                    else:\n",
        "                        # For other layer types, use a simpler propagation rule\n",
        "                        relevances[id(m)] = (a * grad_out[0]).data\n",
        "        \n",
        "        # Register hooks for all eligible modules\n",
        "        if isinstance(module, (nn.Conv2d, nn.Linear, nn.BatchNorm2d, nn.ReLU)):\n",
        "            forward_hooks.append(module.register_forward_hook(forward_hook))\n",
        "            backward_hooks.append(module.register_full_backward_hook(backward_hook))\n",
        "        \n",
        "        # Recurse through all children\n",
        "        for child in module.children():\n",
        "            f_hooks, b_hooks = self._register_hooks(child, activations, relevances)\n",
        "            forward_hooks.extend(f_hooks)\n",
        "            backward_hooks.extend(b_hooks)\n",
        "            \n",
        "        return forward_hooks, backward_hooks\n",
        "    \n",
        "    def __call__(self, input_tensor, target_class=None):\n",
        "        \"\"\"\n",
        "        Generates the LRP heatmap for the input tensor\n",
        "        \n",
        "        Args:\n",
        "            input_tensor: Input image (must be normalized the same way as training data)\n",
        "            target_class: Target class index. If None, uses the predicted class.\n",
        "            \n",
        "        Returns:\n",
        "            relevance_map: The normalized LRP heatmap\n",
        "        \"\"\"\n",
        "        input_tensor = input_tensor.clone().detach().to(device)\n",
        "        input_tensor.requires_grad = True\n",
        "        \n",
        "        # Storage for activations and relevances\n",
        "        activations = {}\n",
        "        relevances = {}\n",
        "        \n",
        "        # Register hooks\n",
        "        forward_hooks, backward_hooks = self._register_hooks(self.model, activations, relevances)\n",
        "        \n",
        "        try:\n",
        "            # Forward pass\n",
        "            output = self.model(input_tensor)\n",
        "            \n",
        "            # If target_class is None, use predicted class\n",
        "            if target_class is None:\n",
        "                target_class = torch.argmax(output, dim=1).item()\n",
        "            \n",
        "            # One-hot encoding for the target class\n",
        "            one_hot = torch.zeros_like(output)\n",
        "            one_hot[0, target_class] = 1.0\n",
        "            \n",
        "            # Backward pass for LRP\n",
        "            self.model.zero_grad()\n",
        "            output.backward(gradient=one_hot, retain_graph=True)\n",
        "            \n",
        "            # Extract the input gradient as the initial relevance map\n",
        "            input_gradient = input_tensor.grad.data\n",
        "            \n",
        "            # Get the relevance map for the first layer (closest to input)\n",
        "            first_layer_id = None\n",
        "            for module in self.model.modules():\n",
        "                if isinstance(module, nn.Conv2d):\n",
        "                    first_layer_id = id(module)\n",
        "                    break\n",
        "            \n",
        "            if first_layer_id in relevances:\n",
        "                relevance_map = relevances[first_layer_id]\n",
        "            else:\n",
        "                # Fallback to input gradient if we can't get the relevance map\n",
        "                relevance_map = input_gradient\n",
        "                \n",
        "            # Sum across channels\n",
        "            relevance_map = relevance_map.sum(dim=1).squeeze()\n",
        "            \n",
        "            # Normalize to 0-1\n",
        "            relevance_map = torch.abs(relevance_map)\n",
        "            if torch.max(relevance_map) > 0:\n",
        "                relevance_map = relevance_map / torch.max(relevance_map)\n",
        "            \n",
        "            return relevance_map.cpu().numpy()\n",
        "            \n",
        "        finally:\n",
        "            # Remove all hooks\n",
        "            for hook in forward_hooks + backward_hooks:\n",
        "                hook.remove()\n",
        "\n",
        "def apply_lrp(model, img_tensor, img_np, target_class=None):\n",
        "    \"\"\"\n",
        "    Applies LRP to visualize model contributions\n",
        "    \n",
        "    Args:\n",
        "        model: Trained PyTorch model\n",
        "        img_tensor: Input image tensor (1, C, H, W)\n",
        "        img_np: Original numpy image for visualization (RGB)\n",
        "        target_class: Target class for visualization\n",
        "        \n",
        "    Returns:\n",
        "        visualization: Heatmap overlaid on original image\n",
        "        relevance_map: Raw relevance map\n",
        "    \"\"\"\n",
        "    # Create LRP instance\n",
        "    lrp = LRP(model)\n",
        "    \n",
        "    # Generate relevance map\n",
        "    relevance_map = lrp(img_tensor, target_class)\n",
        "    \n",
        "    # Resize relevance map to input image size\n",
        "    relevance_resized = cv2.resize(relevance_map, (img_np.shape[1], img_np.shape[0]))\n",
        "    \n",
        "    # Convert to heatmap\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * relevance_resized), cv2.COLORMAP_JET)\n",
        "    \n",
        "    # Convert to RGB (from BGR)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Overlay heatmap on original image\n",
        "    alpha = 0.4\n",
        "    visualization = heatmap * alpha + img_np * (1 - alpha)\n",
        "    visualization = np.uint8(visualization)\n",
        "    \n",
        "    return visualization, relevance_map\n",
        "\n",
        "def visualize_lrp(model, dataloader, class_names, num_images=5):\n",
        "    \"\"\"\n",
        "    Visualizes LRP for a batch of images\n",
        "    \n",
        "    Args:\n",
        "        model: Trained PyTorch model\n",
        "        dataloader: DataLoader containing images to visualize\n",
        "        class_names: Names of the classes\n",
        "        num_images: Number of images to visualize\n",
        "    \"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Get a batch of images\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images = images[:num_images]\n",
        "    labels = labels[:num_images]\n",
        "    \n",
        "    # Create a figure\n",
        "    fig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))\n",
        "    \n",
        "    for i, (image, label) in enumerate(zip(images, labels)):\n",
        "        # Convert to numpy image for display\n",
        "        img_np = image.cpu().numpy().transpose(1, 2, 0)\n",
        "        img_np = np.clip(img_np * np.array([0.229, 0.224, 0.225]) + \n",
        "                        np.array([0.485, 0.456, 0.406]), 0, 1)\n",
        "        \n",
        "        # Prepare input for model\n",
        "        input_tensor = image.unsqueeze(0).to(device)\n",
        "        \n",
        "        # Get model prediction\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "            _, pred = torch.max(output, 1)\n",
        "            prob = torch.nn.functional.softmax(output, dim=1)\n",
        "        \n",
        "        # Generate LRP for true class\n",
        "        true_lrp, _ = apply_lrp(model, input_tensor, img_np, label.item())\n",
        "        \n",
        "        # Generate LRP for predicted class\n",
        "        pred_lrp, _ = apply_lrp(model, input_tensor, img_np, pred.item())\n",
        "        \n",
        "        # Display original image\n",
        "        axes[i, 0].imshow(img_np)\n",
        "        axes[i, 0].set_title(f\"True: {class_names[label]}\\nPred: {class_names[pred]} ({prob[0][pred.item()]:.2f})\")\n",
        "        axes[i, 0].axis('off')\n",
        "        \n",
        "        # Display LRP for true class\n",
        "        axes[i, 1].imshow(true_lrp)\n",
        "        axes[i, 1].set_title(f\"LRP for {class_names[label]}\")\n",
        "        axes[i, 1].axis('off')\n",
        "        \n",
        "        # Display LRP for predicted class\n",
        "        axes[i, 2].imshow(pred_lrp)\n",
        "        axes[i, 2].set_title(f\"LRP for {class_names[pred]}\")\n",
        "        axes[i, 2].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('lrp_visualizations.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "920dcb7f",
      "metadata": {},
      "source": [
        "### Methods Comparison: GradCAM vs LRP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cdac0d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def compare_xai_methods(model, dataloader, class_names, num_images=3):\n",
        "    \"\"\"\n",
        "    Visually compares different XAI methods on the same images\n",
        "    \n",
        "    Args:\n",
        "        model: Trained PyTorch model\n",
        "        dataloader: DataLoader containing images to visualize\n",
        "        class_names: Names of the classes\n",
        "        num_images: Number of images to visualize\n",
        "    \"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Get a batch of images\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images = images[:num_images]\n",
        "    labels = labels[:num_images]\n",
        "    \n",
        "    # Create a figure\n",
        "    fig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))\n",
        "    \n",
        "    for i, (image, label) in enumerate(zip(images, labels)):\n",
        "        # Convert to numpy image for display\n",
        "        img_np = image.cpu().numpy().transpose(1, 2, 0)\n",
        "        img_np = np.clip(img_np * np.array([0.229, 0.224, 0.225]) + \n",
        "                        np.array([0.485, 0.456, 0.406]), 0, 1)\n",
        "        \n",
        "        # Prepare input for model\n",
        "        input_tensor = image.unsqueeze(0).to(device)\n",
        "        \n",
        "        # Get model prediction\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "            _, pred = torch.max(output, 1)\n",
        "            prob = torch.nn.functional.softmax(output, dim=1)\n",
        "        \n",
        "        # Generate GradCAM for predicted class\n",
        "        gradcam_vis, _ = apply_gradcam(model, input_tensor, img_np, pred.item())\n",
        "        \n",
        "        # Generate LRP for predicted class\n",
        "        lrp_vis, _ = apply_lrp(model, input_tensor, img_np, pred.item())\n",
        "        \n",
        "        # Display original image\n",
        "        axes[i, 0].imshow(img_np)\n",
        "        axes[i, 0].set_title(f\"Original\\nTrue: {class_names[label]}\\nPred: {class_names[pred]} ({prob[0][pred.item()]:.2f})\")\n",
        "        axes[i, 0].axis('off')\n",
        "        \n",
        "        # Display GradCAM\n",
        "        axes[i, 1].imshow(gradcam_vis)\n",
        "        axes[i, 1].set_title(\"GradCAM\")\n",
        "        axes[i, 1].axis('off')\n",
        "        \n",
        "        # Display LRP\n",
        "        axes[i, 2].imshow(lrp_vis)\n",
        "        axes[i, 2].set_title(\"Layer-wise Relevance Propagation\")\n",
        "        axes[i, 2].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('xai_comparison.png')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
