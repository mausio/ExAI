{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "GSjCu3qxLzNK",
   "metadata": {
    "id": "GSjCu3qxLzNK"
   },
   "source": [
    "# ExAI - Explainable Corgi (Cardigan) Separator 🐶\n",
    "\n",
    "We use [Contrastive GradCAM](https://xai-blog.netlify.app/docs/groups/contrastive-grad-cam-consistency/#contrastive-grad-cam-consistency-loss)\n",
    "and [Layerwise Relevance Propagation](https://github.com/kaifishr/PyTorchRelevancePropagation) to explain the difference between Corgis and Cardigans - two breeds that are often difficult to distinguish visually even for dog experts.\n",
    "\n",
    "Key visual differences include tail length (Cardigans have longer tails), ear size (Cardigans have larger ears), and body length (Cardigans typically have longer bodies). Our XAI techniques aim to determine if these are indeed the features our model focuses on when making classifications.\n",
    "\n",
    "- We leverage [Stanford ImageNet Dog Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) for fine-tuning [ResNet50](https://pytorch.org/hub/pytorch_vision_resnet/#model-description).\n",
    "- Target breeds: [Pembroke Welsh Corgi](https://de.wikipedia.org/wiki/Welsh_Corgi_Pembroke) | [Cardigan Welsh Corgi](https://de.wikipedia.org/wiki/Welsh_Corgi_Cardigan)\n",
    "\n",
    "## The Process:\n",
    "1. Load the dataset and split it into training (80%) and validation (20%) sets.\n",
    "2. Fine-tune a pre-trained ResNet50 model using transfer learning (freezing early layers).\n",
    "3. Evaluate model performance through accuracy metrics and confusion matrix.\n",
    "4. Apply two XAI techniques to visualize decision factors:\n",
    "   - **GradCAM**: Highlights regions that most influenced the class prediction\n",
    "   - **LRP**: Provides pixel-level relevance scores for the entire image\n",
    "5. Compare both techniques to understand if the model focuses on breed-specific anatomical features.\n",
    "6. Analyze whether our model's reasoning aligns with established breed characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dJH_W2oaOJoB",
   "metadata": {
    "id": "dJH_W2oaOJoB"
   },
   "source": [
    "## 1. Data/Dependency Loading and Transformation\n",
    "\n",
    "This section covers the foundational setup for our Corgi classification pipeline, including:\n",
    "- Import of essential libraries and dependencies\n",
    "- Data acquisition from Stanford Dogs Dataset\n",
    "- Custom dataset class implementation for Corgi breeds\n",
    "- Data augmentation and preprocessing transformations\n",
    "- Creation of training and validation data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11000ff2",
   "metadata": {},
   "source": [
    "### Import of all necessary packages/libraries \n",
    "\n",
    "We import PyTorch and related libraries for deep learning, along with NumPy, Matplotlib, and other data processing tools. These packages enable us to build our classification pipeline, handle image data, train our CNN model, and visualize the XAI results for comparing Pembroke and Cardigan Welsh Corgis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ae81c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy matplotlib seaborn tqdm Pillow opencv-python torch torchvision scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6bPBrZtQvR0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6bPBrZtQvR0",
    "is_executing": true,
    "outputId": "6501edec-c30f-4a80-b3ce-128ff28b9c00"
   },
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import os\n",
    "import time #time measurement and delays \n",
    "import copy \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import seaborn as sns # statistical data visualization\n",
    "from tqdm import tqdm # progress bars for loops\n",
    "from PIL import Image # image loading\n",
    "import cv2 #image processing \n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn # neural network layers\n",
    "import torch.optim as optim # optimization algorithms\n",
    "from torch.optim import lr_scheduler # learning rate scheduling\n",
    "from torch.utils.data import Dataset, DataLoader, random_split # dataset handling\n",
    "import torchvision # for image transformations\n",
    "from torchvision import transforms, models # pre-trained models\n",
    "from torchvision.datasets.utils import download_url, extract_archive # downloading datasets\n",
    "\n",
    "# For evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report # for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932de5c0",
   "metadata": {},
   "source": [
    "### Setting device for GPU acceleration\n",
    "\n",
    "We configure PyTorch to utilize available GPU resources through CUDA, significantly accelerating the training process and matrix operations. If no GPU is available, the code automatically falls back to CPU processing, ensuring compatibility across different hardware environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "SJYyGFcNMcRI",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "SJYyGFcNMcRI",
    "outputId": "2d1015e3-417b-4d9d-a47e-54b95ffc60f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Setup device for training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018e2423",
   "metadata": {},
   "source": [
    "### Downloading Dataset if not already in directory\n",
    "[Stanford Dogs Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/):\n",
    "\n",
    "> The Stanford Dogs dataset contains images of 120 breeds of dogs from around the world. This dataset has been built using images and annotation from ImageNet for the task of fine-grained image categorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "initial_id",
    "outputId": "6c3b9c7f-2926-4caf-e2a3-84370e60329b"
   },
   "outputs": [],
   "source": [
    "def download_and_extract_dataset(download_dir, extract_dir):\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "    \n",
    "    # Download the dataset\n",
    "    dataset_url = \"http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\"\n",
    "    filename = os.path.basename(dataset_url)\n",
    "    filepath = os.path.join(download_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        download_url(dataset_url, download_dir)\n",
    "    else:\n",
    "        print(f\"File {filename} already exists in {download_dir}\")\n",
    "    \n",
    "    # Extract the dataset\n",
    "    if not os.path.exists(os.path.join(extract_dir, \"Images\")):\n",
    "        print(f\"Extracting {filename} to {extract_dir}...\")\n",
    "        extract_archive(filepath, extract_dir)\n",
    "    else:\n",
    "        print(f\"Dataset already extracted to {extract_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84b374a",
   "metadata": {},
   "source": [
    "### Dataset Classes\n",
    "Our dataset classes represent a specific Dataset entity,\n",
    "which is being attributed with a variety of additional attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca1af7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CorgiDataset(Dataset):\n",
    "    def __init__(self, dataset_root, transform=None):\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_names = ['Pembroke', 'Cardigan']\n",
    "        \n",
    "        images_dir = os.path.join(dataset_root, \"Images\")\n",
    "        if not os.path.exists(images_dir):\n",
    "            raise FileNotFoundError(f\"Images directory not found at {images_dir}\")\n",
    "            \n",
    "        all_breeds = os.listdir(images_dir)\n",
    "        \n",
    "        pembroke_dir = None\n",
    "        cardigan_dir = None\n",
    "        \n",
    "        for breed in all_breeds:\n",
    "            if \"Pembroke\" in breed:\n",
    "                pembroke_dir = os.path.join(images_dir, breed)\n",
    "            elif \"Cardigan\" in breed:\n",
    "                cardigan_dir = os.path.join(images_dir, breed)\n",
    "        \n",
    "        if not pembroke_dir or not cardigan_dir:\n",
    "            raise ValueError(\"Could not find Pembroke or Cardigan directories\")\n",
    "        \n",
    "        print(f\"Pembroke directory: {pembroke_dir}\")\n",
    "        print(f\"Cardigan directory: {cardigan_dir}\")\n",
    "        \n",
    "        # Load Pembroke images as label with idx 0\n",
    "        for img_name in os.listdir(pembroke_dir):\n",
    "            if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                self.image_paths.append(os.path.join(pembroke_dir, img_name))\n",
    "                self.labels.append(0)  # Pembroke\n",
    "        \n",
    "        # Load Cardigan images as label with idx 1\n",
    "        for img_name in os.listdir(cardigan_dir):\n",
    "            if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                self.image_paths.append(os.path.join(cardigan_dir, img_name))\n",
    "                self.labels.append(1)  # Cardigan\n",
    "        \n",
    "        print(f\"Total number of images: {len(self.image_paths)}\")\n",
    "        print(f\"Pembroke images: {sum(1 for label in self.labels if label == 0)}\")\n",
    "        print(f\"Cardigan images: {sum(1 for label in self.labels if label == 1)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            label = self.labels[idx]\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a blank image and the same label\n",
    "            blank_image = torch.zeros((3, 224, 224)) if self.transform else Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "            return blank_image, self.labels[idx]\n",
    "        \n",
    "        \n",
    "# Is required to evaluate single image paths\n",
    "class SingleImageDataset(Dataset):\n",
    "    def __init__(self, image_path='./assets/corgi-mischling.jpg', transform=None):\n",
    "        \"\"\"\n",
    "        Dataset for loading a single image without a label\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the image file\n",
    "            transform: PyTorch transforms for preprocessing\n",
    "        \"\"\"\n",
    "        self.image_path = image_path\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1  # Only one image\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(self.image_path).convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, -1  # -1 as a placeholder label since we don't have one\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {self.image_path}: {e}\")\n",
    "            # Return a blank image\n",
    "            blank_image = torch.zeros((3, 224, 224)) if self.transform else Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "            return blank_image, -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a38e9",
   "metadata": {},
   "source": [
    "### Class for transforming Subset\n",
    "\n",
    "When splitting data into training and validation sets, we get Subset objects that don't directly support transformations. \n",
    "\n",
    "TransformedSubset enables us to apply different data strategies to training data versus validation data, which is essential to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48J5ZvoSk-Om",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48J5ZvoSk-Om",
    "outputId": "558113be-691d-477d-b756-670b7e5bc477"
   },
   "outputs": [],
   "source": [
    "class TransformedSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Correctly handle the subset indexing\n",
    "        image, label = self.subset.dataset[self.subset.indices[idx]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783f4d6",
   "metadata": {},
   "source": [
    "### Data Preparation and Loaders\n",
    "\n",
    "This section creates our data pipeline for efficient model training. It has different functions:\n",
    "1. **Data Transformations**: Defines separate strategies for training.\n",
    "2. **Dataset Splitting**: Creates an 80/20 train/validation split.\n",
    "3. **Optimized Loading**: Configures DataLoaders with batch processing, etc.\n",
    "\n",
    "This approach ensures our model trains on varied examples while being evaluated on consistent, unmodified validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fba4851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(dataset_root, batch_size=32, num_workers=2):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    dataset = CorgiDataset(dataset_root, transform=None)  # No transform yet\n",
    "    \n",
    "    train_size = int(0.8 * len(dataset))  # 80%\n",
    "    val_size = len(dataset) - train_size  # 20%\n",
    "    \n",
    "    # Todo: Set random seed for i-reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_dataset_transformed = TransformedSubset(train_dataset, data_transforms['train'])\n",
    "    val_dataset_transformed = TransformedSubset(val_dataset, data_transforms['val'])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset_transformed, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset_transformed, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {len(train_dataset)} images\")\n",
    "    print(f\"Validation set size: {len(val_dataset)} images\")\n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    return train_loader, val_loader, dataset.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s37ot4N9OQGQ",
   "metadata": {
    "id": "s37ot4N9OQGQ"
   },
   "source": [
    "## 2. Model Definition\n",
    "\n",
    "This section implements the transfer learning approach using ResNet50:\n",
    "\n",
    "1. **Load Pre-trained Model**: We load ResNet50 with its pre-trained weights on ImageNet\n",
    "2. **Layer Freezing Strategy**: We employ a strategic freezing pattern of all but the last layer, bcs. it seems to be best-practice.\n",
    "3. **Custom Classification Head**: The original fully-connected layer is replaced with ReLU (Rectified Linear Unit)\n",
    "\n",
    "This approach dramatically reduces training time and required data while maintaining high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7441e4",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X71aR-0Y5O-7",
   "metadata": {
    "cellView": "form",
    "id": "X71aR-0Y5O-7"
   },
   "outputs": [],
   "source": [
    "\n",
    "def setup_model(num_classes=2):\n",
    "    model = models.resnet50(weights='DEFAULT')\n",
    "\n",
    "    # Freeze all layers    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze last layer\n",
    "    for param in model.layer4.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model information\n",
    "    print(f\"ResNet50 model configured for {num_classes} classes\")\n",
    "    parameters_of_layer4 = sum(p.numel() for p in model.layer4.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters in output layer 'model.layer4': {parameters_of_layer4}\")\n",
    "    parameters_of_fully_connected_layers = sum(p.numel() for p in model.fc.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters in fully connected layers 'model.fc': {parameters_of_fully_connected_layers}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cef616",
   "metadata": {},
   "source": [
    "### Training Functions\n",
    "\n",
    "This section contains the core training pipeline components that manage the model training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b4b0b",
   "metadata": {},
   "source": [
    "#### Training Function: Epoch\n",
    "\n",
    "This function handles a single training iteration through the entire dataset. For each batch of images:\n",
    "1. It transfers data to GPU/CPU and zeroes gradients\n",
    "2. Performs predictions \n",
    "3. Calculates loss using cross-entropy (loss function)\n",
    "4. Executes backward propagation to compute gradients\n",
    "5. Updates model weights via optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "o-2VF4oKDqRw",
   "metadata": {
    "id": "o-2VF4oKDqRw"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "    \n",
    "    print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    \n",
    "    return epoch_loss, epoch_acc.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f1783f",
   "metadata": {},
   "source": [
    "#### Training Function: Validate Epoch\n",
    "\n",
    "Evaluates model performance on validation data without updating weights, collecting statistical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "TssTfUXeH_C_",
   "metadata": {
    "id": "TssTfUXeH_C_"
   },
   "outputs": [],
   "source": [
    "\n",
    "def validate_epoch(model, dataloader, criterion):\n",
    "    model.eval()  # Set model to evaluate mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    # Iterate over data\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "    \n",
    "    print(f'Validation Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    \n",
    "    return epoch_loss, epoch_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d3918",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "This orchestration function manages the entire training cycle. It implements:\n",
    "\n",
    "1. **Setting Up the Learning Process**: We use CrossEntropy and configure different learning speeds for different parts of the network\n",
    "2. **Adaptive Learning Rate**: our scheduler reduces learning rates when improvement slows down\n",
    "3. **Training Loop**: The main training cycle runs through our data multiple times (epochs), tracking both how well we're memorizing training data and how well we generalize to new images \n",
    "4. **Smart Quitting**: We stop training when validation accuracy doesn't improve for 5 consecutive epochs - preventing the model from just memorizing training examples and wasting our time.\n",
    "5. **Saving the Best Version**: Rather than keeping the final model, we save a copy of the model weights whenever it achieves a new high score on validation data. In the end the best epoch is being selected. \n",
    "\n",
    "This approach maximizes efficiency by preventing overfitting while ensuring the model reaches optimal performance for our Corgi classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "419a7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=5, patience=4):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.layer4.parameters(), 'lr': 1e-4},\n",
    "        {'params': model.fc.parameters(), 'lr': 1e-3}\n",
    "    ], weight_decay=1e-5)\n",
    "    \n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "    \n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    no_improve_epochs = 0\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 40)\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "        \n",
    "        print(f'Best val Acc: {best_acc:.4f}')\n",
    "        \n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs without improvement')\n",
    "            break\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e3395",
   "metadata": {},
   "source": [
    "### Evaluating Model\n",
    "\n",
    "This function tests how well our model recognizes different Corgis:\n",
    "\n",
    "1. It runs validation images through the model and compares predictions to actual labels\n",
    "2. Creates a colorful grid (confusion matrix) showing correct guesses vs. mistakes\n",
    "3. Calculates accuracy scores for each breed\n",
    "4. Returns all results for analysis\n",
    "\n",
    "Basically, it's like giving our model a final exam and creating a detailed report card!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d52031ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, dataloader, class_names):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    return y_true, y_pred, report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d22702",
   "metadata": {},
   "source": [
    "### Plotting the History\n",
    "\n",
    "This function visualizes two key metrics:\n",
    "1. **Loss trends** - Shows how the error decreases during training\n",
    "2. **Accuracy trends** - Shows how prediction accuracy improves\n",
    "\n",
    "Both metrics are plotted for training and validation data, helping identify when the model starts overfitting (when validation metrics worsen while training metrics continue to improve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d76652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c1883",
   "metadata": {},
   "source": [
    "### Saving the Model\n",
    "\n",
    "Will be tried in all of the three following formats:\n",
    " - **.pth** - for future fine-tuning & xAI.\n",
    " - *.onnx - for possible runtime inference.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5d71d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(model, save_path, class_names, optimizer=None, epoch=None, history=None):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    torch.save(model, save_path.replace('.pth', '_full.pth'))\n",
    "    print(f\"Complete model saved to: {save_path.replace('.pth', '_full.pth')}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), save_path.replace('.pth', '_weights.pth'))\n",
    "    print(f\"Model weights saved to: {save_path.replace('.pth', '_weights.pth')}\")\n",
    "    \n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'classes': class_names,\n",
    "    }\n",
    "    \n",
    "    if optimizer:\n",
    "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
    "    \n",
    "    if epoch is not None:\n",
    "        checkpoint['epoch'] = epoch\n",
    "        \n",
    "    if history:\n",
    "        checkpoint['history'] = history\n",
    "    \n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Checkpoint saved to: {save_path}\")\n",
    "    \n",
    "    try:\n",
    "        dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            save_path.replace('.pth', '.onnx'),\n",
    "            export_params=True,\n",
    "            opset_version=11,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "        )\n",
    "        print(f\"ONNX model saved to: {save_path.replace('.pth', '.onnx')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting to ONNX format: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb0f01",
   "metadata": {},
   "source": [
    "### Loading the Model\n",
    "..from a file which is only applicable to **.pth** files with this function, which does return a tuple (model | None, checkpoint | None) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98c4808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(load_path, model=None):\n",
    "    try:\n",
    "        checkpoint = torch.load(load_path, map_location=device)\n",
    "        \n",
    "        if model is None:\n",
    "            # Try loading the full model\n",
    "            if load_path.endswith('_full.pth'):\n",
    "                model = torch.load(load_path, map_location=device)\n",
    "                print(f\"Full model loaded from: {load_path}\")\n",
    "                return model, None\n",
    "            \n",
    "            # Otherwise create a new model\n",
    "            model = setup_model()\n",
    "        \n",
    "        # Load state dict if it exists\n",
    "        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        \n",
    "        print(f\"Model weights loaded from: {load_path}\")\n",
    "        return model, checkpoint\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ad237",
   "metadata": {},
   "source": [
    "## 3. xAI Methods \n",
    "\n",
    "We chose GradCAM and Layer-wise Relevance Propagation (LRP)\n",
    "to evaluate the decision making criterion with CAM on top of the output layer,\n",
    "as well as from deeper layers with LRP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8def829",
   "metadata": {},
   "source": [
    "### GradCAM\n",
    "Implementation for CNN classification visualization.\n",
    "This class implements the Gradient-weighted Class Activation Mapping (Grad-CAM) technique to visualize which parts of an image are important for a CNN's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "530639fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        \"\"\"\n",
    "        Initializes GradCAM with a model and target layer\n",
    "\n",
    "        Args:\n",
    "            model: The trained PyTorch model\n",
    "            target_layer: The convolutional layer to use for generating the CAM\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.hooks = []\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.register_hooks()\n",
    "        self.model.eval()\n",
    "\n",
    "    def register_hooks(self):\n",
    "        \"\"\"Registers forward and backward hooks to the target layer\"\"\"\n",
    "\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0].detach()\n",
    "\n",
    "        # Register the hooks\n",
    "        forward_handle = self.target_layer.register_forward_hook(forward_hook)\n",
    "        backward_handle = self.target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "        # Store the handles for removal later\n",
    "        self.hooks = [forward_handle, backward_handle]\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Removes all registered hooks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "    def __call__(self, input_tensor, target_class=None):\n",
    "        \"\"\"\n",
    "        Generates the Grad-CAM for the input tensor\n",
    "\n",
    "        Args:\n",
    "            input_tensor: Input image (must be normalized the same way as training data)\n",
    "            target_class: Target class index. If None, uses the predicted class.\n",
    "\n",
    "        Returns:\n",
    "            cam: The normalized Grad-CAM heatmap\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        input_tensor = input_tensor.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        # If target_class is None, use predicted class\n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "        # One-hot encoding of the target class\n",
    "        one_hot = torch.zeros_like(output)\n",
    "        one_hot[0, target_class] = 1\n",
    "\n",
    "        # Backward pass to get gradients\n",
    "        output.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "        # Get mean gradients and activations\n",
    "        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])\n",
    "\n",
    "        # Weight the activations by the gradients\n",
    "        for i in range(pooled_gradients.shape[0]):\n",
    "            self.activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "        # Average activations over the channel dimension\n",
    "        cam = torch.mean(self.activations, dim=1).squeeze()\n",
    "\n",
    "        # ReLU on the heatmap\n",
    "        cam = torch.maximum(cam, torch.tensor(0.0).to(device))\n",
    "\n",
    "        # Normalize the heatmap\n",
    "        if torch.max(cam) > 0:\n",
    "            cam = cam / torch.max(cam)\n",
    "\n",
    "        # Resize to the input image size\n",
    "        cam = cam.cpu().numpy()\n",
    "\n",
    "        return cam\n",
    "\n",
    "\n",
    "def apply_gradcam(model, img_tensor, img_np, target_class=None, layer_name=\"layer4\"):\n",
    "    \"\"\"\n",
    "    Applies GradCAM to visualize model attention,\n",
    "    being returned as raw Heatmap.\n",
    "    \"\"\"\n",
    "    # Get the target layer\n",
    "    target_layer = model.layer4\n",
    "\n",
    "    # Create GradCAM instance\n",
    "    grad_cam = GradCAM(model, target_layer)\n",
    "\n",
    "    # Generate heatmap\n",
    "    cam = grad_cam(img_tensor, target_class)\n",
    "\n",
    "    # Resize CAM to input image size\n",
    "    cam_resized = cv2.resize(cam, (img_np.shape[1], img_np.shape[0]))\n",
    "\n",
    "    # Convert to heatmap\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
    "\n",
    "    # Convert to RGB (from BGR)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Overlay heatmap on original image\n",
    "    alpha = 0.4\n",
    "    visualization = heatmap * alpha + img_np * (1 - alpha)\n",
    "    visualization = np.uint8(visualization)\n",
    "\n",
    "    # Remove hooks\n",
    "    grad_cam.remove_hooks()\n",
    "\n",
    "    return visualization, cam\n",
    "\n",
    "\n",
    "def visualize_gradcam(model, dataloader, class_names, num_images=5):\n",
    "    \"\"\"\n",
    "    Visualizes GradCAM for a batch of num_images.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        dataloader: DataLoader containing images to visualize\n",
    "        class_names: Names of the classes\n",
    "        num_images: Number of images to visualize\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get a batch of images\n",
    "    images, labels = next(iter(dataloader))\n",
    "    images = images[:num_images]\n",
    "    labels = labels[:num_images]\n",
    "\n",
    "    # Create a figure\n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))\n",
    "\n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        # Convert to numpy image for display\n",
    "        img_np = image.cpu().numpy().transpose(1, 2, 0)\n",
    "        img_np = np.clip(\n",
    "            img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]),\n",
    "            0,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        # Prepare input for model\n",
    "        input_tensor = image.unsqueeze(0).to(device)\n",
    "\n",
    "        # Get model prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            prob = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "        # Generate GradCAM for true class\n",
    "        true_cam, _ = apply_gradcam(model, input_tensor, img_np, label.item())\n",
    "\n",
    "        # Generate GradCAM for predicted class\n",
    "        pred_cam, _ = apply_gradcam(model, input_tensor, img_np, pred.item())\n",
    "\n",
    "        # Display original image\n",
    "        axes[i, 0].imshow(img_np)\n",
    "        axes[i, 0].set_title(\n",
    "            f\"True: {class_names[label]}\\nPred: {class_names[pred]} ({prob[0][pred.item()]:.2f})\"\n",
    "        )\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        # Display GradCAM for true class\n",
    "        axes[i, 1].imshow(true_cam)\n",
    "        axes[i, 1].set_title(f\"GradCAM for {class_names[label]}\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        # Display GradCAM for predicted class\n",
    "        axes[i, 2].imshow(pred_cam)\n",
    "        axes[i, 2].set_title(f\"GradCAM for {class_names[pred]}\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"gradcam_visualizations.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa647935",
   "metadata": {},
   "source": [
    "### Layer-wise Relevance Propagation (LRP) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f3e702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRP:\n",
    "    \"\"\"\n",
    "    Simplified Layer-wise Relevance Propagation (LRP) for CNN visualization.\n",
    "    \n",
    "    This implementation uses a simpler gradient-based approach that avoids\n",
    "    the issues with tensor views and in-place modifications.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, epsilon=1e-9):\n",
    "        \"\"\"\n",
    "        Initializes LRP with a model\n",
    "        \n",
    "        Args:\n",
    "            model: The trained PyTorch model (ResNet50)\n",
    "            epsilon: Small constant for numerical stability\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.model.eval()\n",
    "    \n",
    "    def __call__(self, input_tensor, target_class=None):\n",
    "        \"\"\"\n",
    "        Generates the LRP heatmap for the input tensor using a simplified approach\n",
    "        \n",
    "        Args:\n",
    "            input_tensor: Input image tensor (must be normalized)\n",
    "            target_class: Target class index. If None, uses the predicted class.\n",
    "            \n",
    "        Returns:\n",
    "            relevance_map: The normalized LRP heatmap\n",
    "        \"\"\"\n",
    "        # Make a detached copy of the input that requires gradient\n",
    "        input_copy = input_tensor.clone().detach().to(device)\n",
    "        input_copy.requires_grad = True\n",
    "        \n",
    "        # Forward pass\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(input_copy)\n",
    "        \n",
    "        # If target_class is None, use predicted class\n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(output, dim=1).item()\n",
    "        \n",
    "        # One-hot encoding for the target class\n",
    "        one_hot = torch.zeros_like(output)\n",
    "        one_hot[0, target_class] = 1.0\n",
    "        \n",
    "        # Backward pass to get gradients\n",
    "        output.backward(gradient=one_hot)\n",
    "        \n",
    "        # Get the gradient with respect to the input\n",
    "        # This represents how much each input pixel affects the output\n",
    "        grad = input_copy.grad.clone()\n",
    "        \n",
    "        # Element-wise product of input and gradient\n",
    "        # This gives us a relevance map highlighting important features\n",
    "        relevance = (input_copy * grad).sum(dim=1).squeeze()\n",
    "        \n",
    "        # Take absolute value and normalize\n",
    "        relevance = torch.abs(relevance)\n",
    "        if torch.max(relevance) > 0:\n",
    "            relevance = relevance / torch.max(relevance)\n",
    "        \n",
    "        return relevance.detach().cpu().numpy()\n",
    "\n",
    "def apply_lrp(model, img_tensor, img_np, target_class=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        img_tensor: Input image tensor (1, C, H, W)\n",
    "        img_np: Original numpy image for visualization (RGB)\n",
    "        target_class: Target class for visualization\n",
    "        \n",
    "    Returns:\n",
    "        visualization: Heatmap overlaid on original image\n",
    "        relevance_map: Raw relevance map\n",
    "    \"\"\"\n",
    "    # Create LRP instance\n",
    "    lrp = LRP(model)\n",
    "    \n",
    "    try:\n",
    "        # Generate relevance map\n",
    "        relevance_map = lrp(img_tensor, target_class)\n",
    "        \n",
    "        if relevance_map is None:\n",
    "            # Return a blank heatmap if LRP fails\n",
    "            relevance_map = np.zeros((img_np.shape[0], img_np.shape[1]))\n",
    "            visualization = img_np.copy()\n",
    "            return visualization, relevance_map\n",
    "        \n",
    "        # Resize relevance map to input image size\n",
    "        relevance_resized = cv2.resize(relevance_map, (img_np.shape[1], img_np.shape[0]))\n",
    "        \n",
    "        # Convert to heatmap\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * relevance_resized), cv2.COLORMAP_JET)\n",
    "        \n",
    "        # Convert to RGB (from BGR)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Overlay heatmap on original image\n",
    "        alpha = 0.4\n",
    "        visualization = heatmap * alpha + img_np * (1 - alpha)\n",
    "        visualization = np.uint8(visualization)\n",
    "        \n",
    "        return visualization, relevance_map\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LRP computation: {e}\")\n",
    "        # Return a blank heatmap if LRP fails\n",
    "        relevance_map = np.zeros((img_np.shape[0], img_np.shape[1]))\n",
    "        visualization = img_np.copy()\n",
    "        return visualization, relevance_map\n",
    "\n",
    "\n",
    "def visualize_lrp(model, dataloader, class_names, num_images=5):\n",
    "    \"\"\"\n",
    "        model: Trained PyTorch model\n",
    "        dataloader: DataLoader containing images to visualize\n",
    "        class_names: Names of the classes\n",
    "        num_images: Number of images to visualize\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get a batch of images\n",
    "    images, labels = next(iter(dataloader))\n",
    "    images = images[:num_images]\n",
    "    labels = labels[:num_images]\n",
    "\n",
    "    # Create a figure\n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))\n",
    "\n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        # Convert to numpy image for display\n",
    "        img_np = image.cpu().numpy().transpose(1, 2, 0)\n",
    "        img_np = np.clip(\n",
    "            img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]),\n",
    "            0,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        # Prepare input for model\n",
    "        input_tensor = image.unsqueeze(0).to(device)\n",
    "\n",
    "        # Get model prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            prob = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "        # Generate LRP for true class\n",
    "        true_lrp, _ = apply_lrp(model, input_tensor, img_np, label.item())\n",
    "\n",
    "        # Generate LRP for predicted class\n",
    "        pred_lrp, _ = apply_lrp(model, input_tensor, img_np, pred.item())\n",
    "\n",
    "        # Display original image\n",
    "        axes[i, 0].imshow(img_np)\n",
    "        axes[i, 0].set_title(\n",
    "            f\"True: {class_names[label]}\\nPred: {class_names[pred]} ({prob[0][pred.item()]:.2f})\"\n",
    "        )\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        # Display LRP for true class\n",
    "        axes[i, 1].imshow(true_lrp)\n",
    "        axes[i, 1].set_title(f\"LRP for {class_names[label]}\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        # Display LRP for predicted class\n",
    "        axes[i, 2].imshow(pred_lrp)\n",
    "        axes[i, 2].set_title(f\"LRP for {class_names[pred]}\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"lrp_visualizations.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920dcb7f",
   "metadata": {},
   "source": [
    "### Methods Comparison: GradCAM vs LRP\n",
    "\n",
    "Visually compares different XAI methods on the same images,\n",
    "by putting different maps side by side nexto the original image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cdac0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_xai_methods(model, dataloader, class_names, num_images=3):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        model: Trained PyTorch model\n",
    "        dataloader: DataLoader containing images to visualize\n",
    "        class_names: Names of the classes\n",
    "        num_images: Number of images to visualize\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get a batch of images\n",
    "    images, labels = next(iter(dataloader))\n",
    "    images = images[:num_images]\n",
    "    labels = labels[:num_images]\n",
    "\n",
    "    # Create a figure\n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))\n",
    "\n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        # Convert to numpy image for display\n",
    "        img_np = image.cpu().numpy().transpose(1, 2, 0)\n",
    "        img_np = np.clip(\n",
    "            img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]),\n",
    "            0,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        # Prepare input for model\n",
    "        input_tensor = image.unsqueeze(0).to(device)\n",
    "\n",
    "        # Get model prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            prob = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "        # Generate GradCAM for predicted class\n",
    "        gradcam_vis, _ = apply_gradcam(model, input_tensor, img_np, pred.item())\n",
    "\n",
    "        # Generate LRP for predicted class\n",
    "        lrp_vis, _ = apply_lrp(model, input_tensor, img_np, pred.item())\n",
    "\n",
    "        # Display original image\n",
    "        axes[i, 0].imshow(img_np)\n",
    "        axes[i, 0].set_title(\n",
    "            f\"Original\\nTrue: {class_names[label]}\\nPred: {class_names[pred]} ({prob[0][pred.item()]:.2f})\"\n",
    "        )\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        # Display GradCAM\n",
    "        axes[i, 1].imshow(gradcam_vis)\n",
    "        axes[i, 1].set_title(\"GradCAM\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        # Display LRP\n",
    "        axes[i, 2].imshow(lrp_vis)\n",
    "        axes[i, 2].set_title(\"Layer-wise Relevance Propagation\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"xai_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Add a description to detailed analysis\n",
    "    print(\"\\nComparison:\")\n",
    "    print(\"  - GradCAM tends to highlight broader regions.\")\n",
    "    print(\"  - LRP often produces more detailed and precise feature attributions.\")\n",
    "    print(\n",
    "        \"  - For complex features (like dog breeds), these visualizations help identify\"\n",
    "    )\n",
    "    print(\"    which visual traits the model is using to distinguish between classes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df329a1",
   "metadata": {},
   "source": [
    "## 4. Main Execution\n",
    "Finally, you can run the program, which does:\n",
    "\n",
    " 1. One-Time Download of Stanford Dog Dataset.\n",
    " 2. One-Time Training of ResNet50.\n",
    " 3. One-Time Saving your finetuned CNN in `download_dir`.\n",
    " 4. Load up finetuned CNN for explanation.\n",
    " 5. Visualizing each method first.\n",
    " 6. Visualizing each method side-by-side.\n",
    " 7. Visualizing a rather difficult example of a crossbreed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbeba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File images.tar already exists in ./downloads\n",
      "Dataset already extracted to ./dogs\n",
      "Pembroke directory: ./dogs\\Images\\n02113023-Pembroke\n",
      "Cardigan directory: ./dogs\\Images\\n02113186-Cardigan\n",
      "Total number of images: 336\n",
      "Pembroke images: 181\n",
      "Cardigan images: 155\n",
      "Training set size: 268 images\n",
      "Validation set size: 68 images\n",
      "Training batches: 9\n",
      "Validation batches: 3\n",
      "ResNet50 model configured for 2 classes\n",
      "Trainable parameters in output layer 'model.layer4': 14964736\n",
      "Trainable parameters in fully connected layers 'model.fc': 1050114\n",
      "Loading pre-trained model...\n",
      "ResNet50 model configured for 2 classes\n",
      "Trainable parameters in output layer 'model.layer4': 14964736\n",
      "Trainable parameters in fully connected layers 'model.fc': 1050114\n",
      "Model weights loaded from: ./downloads\\resnet50_corgi_classifier.pth\n",
      "\n",
      "==================================================\n",
      "Applying XAI Methods for Model Interpretability\n",
      "==================================================\n",
      "\n",
      "Comparing GradCAM and LRP methods...\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    download_dir = \"./downloads\"  # @param {type:\"string\"}\n",
    "    extract_dir = \"./dogs\"  # @param {type:\"string\"}\n",
    "    number_of_epochs = 3\n",
    "    images_to_apply_xai_on = 5\n",
    "\n",
    "    download_and_extract_dataset(download_dir, extract_dir)\n",
    "\n",
    "    train_loader, val_loader, class_names = prepare_dataloaders(\n",
    "        extract_dir, batch_size=32\n",
    "    )\n",
    "\n",
    "    model = setup_model(num_classes=len(class_names))\n",
    "\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    model_path = os.path.join(download_dir, \"resnet50_corgi_classifier.pth\")\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading pre-trained model...\")\n",
    "        model, checkpoint = load_model(model_path)\n",
    "    else:\n",
    "        print(\"Training a new model...\")\n",
    "        model, history = train_model(\n",
    "            model, train_loader, val_loader, num_epochs=number_of_epochs\n",
    "        )\n",
    "\n",
    "        plot_training_history(history)\n",
    "\n",
    "        y_true, y_pred, report = evaluate_model(model, val_loader, class_names)\n",
    "\n",
    "        save_model(model, model_path, class_names=class_names, history=history)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Applying XAI Methods for Model Interpretability\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # print(\"\\nGenerating GradCAM visualizations for first...\")\n",
    "    # visualize_gradcam(model, val_loader, class_names, num_images=images_to_apply_xai_on)\n",
    "\n",
    "    # print(\"\\nGenerating Layer-wise Relevance Propagation visualizations...\")\n",
    "    # visualize_lrp(model, val_loader, class_names, num_images=images_to_apply_xai_on)\n",
    "\n",
    "    print(\"\\nComparing GradCAM and LRP methods...\")\n",
    "    # TODO: Does fail! due to workers being unable to woek for some reason..\n",
    "    # ..probably bcs. of jupyter kernel.\n",
    "    compare_xai_methods(\n",
    "        model, val_loader, class_names, num_images=images_to_apply_xai_on\n",
    "    )\n",
    "\n",
    "    print(\"\\nLoading manual image for xAI...\")\n",
    "    manual_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Path to the specific image\n",
    "    manual_image_path = \"./assets/corgi-mischling.jpg\"\n",
    "\n",
    "    # Create dataset and loader for the single image\n",
    "    manual_dataset = SingleImageDataset(manual_image_path, transform=manual_transform)\n",
    "    manual_loader = DataLoader(manual_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    compare_xai_methods(model, manual_loader, class_names, num_images=1)\n",
    "\n",
    "    print(\"\\nXAI visualization complete. All results saved as PNG files.\")\n",
    "\n",
    "    input(\"Press Enter to exit...\")\n",
    "    # TODO: Clean up temporary files if needed\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
